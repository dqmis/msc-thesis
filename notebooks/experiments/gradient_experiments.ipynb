{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80fc4297",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import json\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from src.problems.problems import compute_producer_optimal_solution, compute_consumer_optimal_solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "32189296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_correlation(a, b):\n",
    "    a = a.flatten()\n",
    "    b = b.flatten()\n",
    "\n",
    "    a_mean = np.mean(a)\n",
    "    b_mean = np.mean(b)\n",
    "\n",
    "    numerator = np.sum((a - a_mean) * (b - b_mean))\n",
    "    denominator = np.sqrt(np.sum((a - a_mean) ** 2) * np.sum((b - b_mean) ** 2))\n",
    "\n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f67f0247",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH_ROOT = Path(\"../../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01ac3b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "with open(DATA_PATH_ROOT / \"amazon_predictions.npy\", \"rb\") as f:\n",
    "    REL_MATRIX = np.load(f)\n",
    "\n",
    "with open(DATA_PATH_ROOT / \"amazon_user_groups.json\", \"r\") as f:\n",
    "    GROUPS_MAP = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "be8863af",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_mat = REL_MATRIX[0:200, 0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2a40aa00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9.999999999999998,\n",
       " array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], shape=(200, 200)))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_producer_optimal_solution(\n",
    "    rel_matrix=rel_mat,\n",
    "    k_rec=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4be50f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, mean_allocations = compute_consumer_optimal_solution(\n",
    "    rel_matrix=rel_mat,\n",
    "    k_rec=10,\n",
    "    producer_max_min_utility=10,\n",
    "    gamma=0.5,\n",
    "    method=\"mean\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "828ae27f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9466594812127049)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.sum(mean_allocations * rel_mat, axis=1)) / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08122e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper‐parameters\n",
    "k_rec  = 10\n",
    "u_min  = 10.0\n",
    "gamma  = 0.5\n",
    "prod_min = math.ceil(gamma * u_min)\n",
    "\n",
    "# hyper‐params\n",
    "tau0, tau_min, decay = 1.0, 0.01, 1e-3\n",
    "lr, lr_dual          = 1e-2, 1e-2\n",
    "λ = torch.zeros(n);  μ = torch.zeros(m)\n",
    "\n",
    "for epoch in range(E):\n",
    "    tau     = max(tau_min, tau0 * exp(-decay * epoch))\n",
    "    Z       = z / tau\n",
    "    A       = torch.softmax(Z, dim=1) * k_rec\n",
    "\n",
    "    util    = torch.mean((A*R).sum(1) / k_rec)\n",
    "    col_s   = A.sum(0)\n",
    "    L_prod  = torch.mean(F.relu(prod_min - col_s)**2)\n",
    "    L_bin   = torch.mean(A*(1 - A))\n",
    "\n",
    "    # Augmented Lagrangian\n",
    "    L_prim  = -util \\\n",
    "            + torch.dot(λ,  A.sum(1) - k_rec) \\\n",
    "            + torch.dot(μ,  F.relu(prod_min - col_s)) \\\n",
    "            + γ * L_bin\n",
    "\n",
    "    opt.zero_grad()\n",
    "    L_prim.backward()\n",
    "    opt.step()\n",
    "\n",
    "    # dual ascent\n",
    "    with torch.no_grad():\n",
    "        λ += lr_dual * (A.sum(1) - k_rec)\n",
    "        μ += lr_dual * F.relu(prod_min - col_s)\n",
    "\n",
    "    # optional STE rounding every so often\n",
    "    if epoch > warmup and epoch % project_every == 0:\n",
    "        A_hard = top_k_allocations(A.detach().cpu().numpy(), k_rec)\n",
    "        z      = z + (torch.tensor(A_hard) - A).to(z.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "84c14ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1 | util: 0.8922 | L_prod: 0.0000 | L_bin: 0.0475\n",
      "Epoch  500 | util: 0.9102 | L_prod: 0.0000 | L_bin: 0.0466\n",
      "Epoch 1000 | util: 0.9093 | L_prod: 0.0000 | L_bin: 0.0410\n",
      "Epoch 1500 | util: 0.8990 | L_prod: 0.0000 | L_bin: 0.0196\n",
      "Discrete allocations shape: (200, 200)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# === hyper‐parameters ===\n",
    "tau0         = 1.0        # initial softmax “temperature”\n",
    "tau_min      = 0.01       # minimum temperature\n",
    "decay        = 1e-3       # temperature decay rate per epoch\n",
    "lr           = 1e-2       # primal (z) learning rate\n",
    "lr_dual      = 1e-2       # dual (λ, μ) learning rate\n",
    "num_epochs   = 1500       # total training epochs\n",
    "warmup_frac  = 0.5        # fraction of epochs before we start STE projection\n",
    "project_every= 100        # how often to project (straight‐through)\n",
    "# ========================\n",
    "\n",
    "def top_k_allocations(allocations: np.ndarray, k_rec: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Given a continuous allocation matrix, returns the hard 0/1 top‐k per row.\n",
    "    \"\"\"\n",
    "    idxs = allocations.argsort(axis=1)[:, -k_rec:]\n",
    "    alls = np.zeros_like(allocations, dtype=np.int32)\n",
    "    alls[np.arange(allocations.shape[0])[:, None], idxs] = 1\n",
    "    return alls\n",
    "\n",
    "def optim_augmented(\n",
    "    rel_matrix: np.ndarray,\n",
    "    k_rec: int,\n",
    "    prod_min: float,\n",
    "    gamma: float,\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Augmented‐Lagrangian + row‐softmax relaxation for\n",
    "    your Boolean LP:\n",
    "      max mean_i ∑_j a_ij R_ij\n",
    "      s.t. ∑_j a_ij = k_rec,   ∑_i a_ij ≥ prod_min,   a_ij ∈ {0,1}\n",
    "    Returns:\n",
    "      A_disc (n×m, int): final 0/1 allocations\n",
    "      A_cont (n×m, float): final continuous allocations\n",
    "    \"\"\"\n",
    "    n, m = rel_matrix.shape\n",
    "    R = torch.tensor(rel_matrix, dtype=torch.float32)\n",
    "\n",
    "    # optimization variable\n",
    "    z = torch.zeros(n, m, requires_grad=True)\n",
    "    opt = torch.optim.Adam([z], lr=lr)\n",
    "\n",
    "    # dual multipliers for each constraint\n",
    "    lambda_row = torch.ones(n)  # for ∑_j a_ij = k_rec\n",
    "    mu_col     = torch.ones(m)  # for ∑_i a_ij ≥ prod_min\n",
    "\n",
    "    warmup_epochs = int(warmup_frac * num_epochs)\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        # 1) temperature schedule\n",
    "        tau = max(tau_min, tau0 * math.exp(-decay * epoch))\n",
    "\n",
    "        # 2) row‐softmax to enforce ∑_j a_ij = k_rec\n",
    "        Z = z / tau\n",
    "        A = torch.softmax(Z, dim=1) * k_rec\n",
    "\n",
    "        # 3) compute objectives\n",
    "        util     = torch.mean(torch.sum(A * R, dim=1) / k_rec)\n",
    "        col_sums = torch.sum(A, dim=0)\n",
    "        # hinge‐squared for ∑_i a_ij ≥ prod_min\n",
    "        L_prod   = torch.mean(F.relu(prod_min - col_sums)**2)\n",
    "        # binarity penalty\n",
    "        L_bin    = torch.mean(A * (1 - A))\n",
    "\n",
    "        # 4) augmented Lagrangian (primal loss)\n",
    "        row_res   = torch.sum(A, dim=1) - k_rec\n",
    "        col_res   = F.relu(prod_min - col_sums)\n",
    "        L_prim    = (\n",
    "            -util\n",
    "            + torch.dot(lambda_row,   row_res)\n",
    "            + torch.dot(mu_col,       col_res)\n",
    "            + gamma * L_bin\n",
    "        )\n",
    "\n",
    "        # 5) gradient step on z\n",
    "        opt.zero_grad()\n",
    "        L_prim.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # 6) dual ascent\n",
    "        with torch.no_grad():\n",
    "            lambda_row += lr_dual * row_res\n",
    "            mu_col     += lr_dual * col_res\n",
    "\n",
    "        # 7) Straight‐through “hardening” every so often\n",
    "        if epoch > warmup_epochs and epoch % project_every == 0:\n",
    "            A_hard = top_k_allocations(A.detach().cpu().numpy(), k_rec)\n",
    "            A_hard_t = torch.tensor(A_hard, dtype=torch.float32)\n",
    "            with torch.no_grad():\n",
    "                z += (A_hard_t - A)  # STE update\n",
    "\n",
    "        # (optional) logging\n",
    "        if epoch % 500 == 0 or epoch == 1:\n",
    "            print(f\"Epoch {epoch:4d} | util: {util.item():.4f} \"\n",
    "                  f\"| L_prod: {L_prod.item():.4f} | L_bin: {L_bin.item():.4f}\")\n",
    "\n",
    "    # Final continuous allocation at lowest temperature\n",
    "    Z_final = z / tau_min\n",
    "    A_cont  = torch.softmax(Z_final, dim=1) * k_rec\n",
    "    A_cont_np = A_cont.detach().cpu().numpy()\n",
    "    A_disc    = top_k_allocations(A_cont_np, k_rec)\n",
    "\n",
    "    return A_disc, A_cont_np\n",
    "\n",
    "\n",
    "\n",
    "disc, cont = optim_augmented(rel_matrix=rel_mat,\n",
    "                                 k_rec=10,\n",
    "                                 prod_min=math.ceil(0.5 * 10.0),\n",
    "                                 gamma=0.05)\n",
    "print(\"Discrete allocations shape:\", disc.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "id": "f4753f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   500 — loss: 6914.6748, util: 0.58, card: 38.6275, prod: 3.0577, bin: 0.0001\n",
      "Epoch  1000 — loss: 5828.4238, util: 0.58, card: 35.0871, prod: 2.3255, bin: 0.0002\n",
      "Epoch  1500 — loss: 5520.0186, util: 0.58, card: 34.2992, prod: 2.0959, bin: 0.0001\n",
      "Epoch  2000 — loss: 5167.5361, util: 0.58, card: 33.8881, prod: 1.7845, bin: 0.0001\n",
      "Epoch  2500 — loss: 4904.4922, util: 0.58, card: 31.3538, prod: 1.7749, bin: 0.0001\n",
      "Epoch  3000 — loss: 4719.0176, util: 0.58, card: 29.7777, prod: 1.7470, bin: 0.0001\n",
      "Epoch  3500 — loss: 4655.2881, util: 0.58, card: 29.8909, prod: 1.6720, bin: 0.0001\n",
      "Epoch  4000 — loss: 4400.0166, util: 0.58, card: 30.4515, prod: 1.3607, bin: 0.0001\n",
      "Epoch  4500 — loss: 4322.3301, util: 0.58, card: 29.3516, prod: 1.3929, bin: 0.0001\n",
      "Epoch  5000 — loss: 4288.4893, util: 0.57, card: 28.2400, prod: 1.4702, bin: 0.0002\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class TwoLayerSelector(nn.Module):\n",
    "    def __init__(self, n_users, hidden_dim, n_items):\n",
    "        super().__init__()\n",
    "        # first “layer”: maps each user to a hidden representation\n",
    "        self.z1 = nn.Parameter(torch.randn(n_users, hidden_dim))\n",
    "        # second “layer”: maps hidden reps into item-scores\n",
    "        self.z2 = nn.Parameter(torch.randn(hidden_dim, n_items))\n",
    "\n",
    "    def forward(self):\n",
    "        # pre-sigmoid logits in item-space:\n",
    "        logits = self.z1 @ self.z2       # shape (n_users, n_items)\n",
    "        return torch.sigmoid(logits)     # a ∈ (0,1)^{n×m}\n",
    "\n",
    "R = torch.tensor(rel_mat)\n",
    "n, m = R.shape    # for example\n",
    "hidden_dim = 200       # you choose\n",
    "model = TwoLayerSelector(n, hidden_dim, m)\n",
    "\n",
    "opt = optim.Adam(model.parameters(), lr=0.1)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(opt, patience=100, factor=0.5, verbose=True)\n",
    "\n",
    "# your constants…\n",
    "lambda_util, lambda_card, lambda_prod = 10, 100, 1000\n",
    "k_rec, decay = 10, 1\n",
    "R = torch.rand(n, m)          # whatever rewards matrix\n",
    "prod_min = torch.ones(m) * 5  # example\n",
    "\n",
    "for epoch in range(1, 5001):\n",
    "    a = model()  # now two-layer rather than single z\n",
    "\n",
    "    # === exactly the same losses as before ===\n",
    "    util   = (a * R).sum(dim=1).mean() / k_rec\n",
    "    L_util = -util\n",
    "\n",
    "    row_sums = a.sum(dim=1)\n",
    "    L_card   = ((row_sums - k_rec)**2).mean()\n",
    "\n",
    "    col_sums = a.sum(dim=0)\n",
    "    L_prod   = torch.relu(prod_min - col_sums).pow(2).mean()\n",
    "\n",
    "    L_bin    = (a * (1 - a)).mean()\n",
    "\n",
    "    loss = (lambda_util * L_util\n",
    "            + lambda_card * L_card\n",
    "            + lambda_prod * L_prod\n",
    "            + L_bin)\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    scheduler.step(loss)\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"Epoch {epoch:5d} — \"\n",
    "              f\"loss: {loss.item():.4f}, \"\n",
    "              f\"util: {util.item():.2f}, \"\n",
    "              f\"card: {L_card.item():.4f}, \"\n",
    "              f\"prod: {L_prod.item():.4f}, \"\n",
    "              f\"bin: {L_bin.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "id": "f0c4ce79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0000000e+00,\n",
       "       1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 9.9999940e-01,\n",
       "       9.9999249e-01, 8.6736975e-08, 3.3401788e-11, 3.4550032e-13,\n",
       "       5.5672348e-14, 3.9424988e-15, 8.4031952e-16, 2.7273479e-16,\n",
       "       5.0404889e-17, 4.6134803e-18, 1.3354737e-18, 9.7296582e-19,\n",
       "       5.0736260e-19, 1.7643549e-19, 7.1620997e-23, 5.5525855e-23,\n",
       "       4.2024223e-23, 1.7201043e-23, 9.5035897e-25, 8.8954332e-25,\n",
       "       6.8952616e-25, 4.6912843e-25, 2.3436868e-25, 1.7843234e-25,\n",
       "       7.1527239e-26, 4.6985175e-26, 1.5371144e-26, 4.8078646e-27,\n",
       "       3.1385659e-27, 1.2347193e-27, 1.1496957e-27, 6.8806948e-28,\n",
       "       1.5683688e-28, 3.9537964e-29, 1.4174376e-29, 7.8123219e-30,\n",
       "       5.1484843e-30, 4.5798904e-30, 3.7377410e-30, 2.1696022e-30,\n",
       "       1.9388040e-30, 1.2963986e-30, 3.6371838e-31, 2.4597840e-31,\n",
       "       7.2596333e-32, 3.5179557e-32, 3.3808114e-32, 1.0104752e-32,\n",
       "       8.8565264e-33, 8.6458227e-33, 4.9887995e-33, 4.1283255e-33,\n",
       "       3.3342027e-33, 2.0958399e-33, 2.0658830e-33, 1.5896937e-33,\n",
       "       8.2514395e-34, 6.3398860e-34, 3.6297351e-34, 2.4823562e-34,\n",
       "       1.0042260e-34, 3.9569226e-35, 1.7655861e-35, 1.5346819e-35,\n",
       "       1.4284196e-35, 8.1841848e-37, 3.5187389e-37, 2.7837216e-37,\n",
       "       1.9131488e-37, 6.2592740e-38, 5.4872051e-38, 4.8931347e-38,\n",
       "       4.5357798e-38, 2.5341709e-38, 2.3199747e-38, 1.9632369e-38,\n",
       "       1.6290093e-38, 1.4933013e-38, 1.2364045e-38, 4.3933594e-39,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 589,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(A_cont[0])[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "id": "51294256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 200)"
      ]
     },
     "execution_count": 583,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_cont = model.forward().detach().cpu().numpy()\n",
    "A_cont.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "id": "317d87dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_cont = model.forward().detach().cpu().numpy()\n",
    "\n",
    "A_opt = np.zeros_like(A_cont)\n",
    "for i, row in enumerate(A_cont):\n",
    "    A_opt[i, row.argsort()[-k_rec:]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "id": "6d538269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.8951087220039388)"
      ]
     },
     "execution_count": 585,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.sum(A_opt * rel_mat, axis=1) / k_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "514429df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dominykas.seputis/github/msc-thesis/.venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   500 — loss: -8.8511, util: 0.89, card: 0.0004, prod: 0.0000, bin: 0.0476\n",
      "Epoch  1000 — loss: -8.8768, util: 0.89, card: 0.0000, prod: 0.0000, bin: 0.0475\n",
      "Epoch  1500 — loss: -8.8769, util: 0.89, card: 0.0000, prod: 0.0000, bin: 0.0475\n",
      "Epoch  2000 — loss: -8.8771, util: 0.89, card: 0.0000, prod: 0.0000, bin: 0.0475\n",
      "Epoch  2500 — loss: -8.8773, util: 0.89, card: 0.0000, prod: 0.0000, bin: 0.0475\n",
      "Epoch  3000 — loss: -8.8776, util: 0.89, card: 0.0000, prod: 0.0000, bin: 0.0475\n"
     ]
    }
   ],
   "source": [
    "\n",
    "R = torch.tensor(rel_mat)\n",
    "n, m = R.shape\n",
    "z = torch.rand(n, m, requires_grad=True, device=\"cpu\")\n",
    "opt = torch.optim.Adam([z], lr=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, patience=100, factor=0.5, verbose=True)\n",
    "prod_min = 5\n",
    "\n",
    "\n",
    "lambda_util = 10\n",
    "lambda_card = 100\n",
    "lambda_prod = 10\n",
    "k_rec = 10\n",
    "decay = 1e-2\n",
    "\n",
    "for epoch in range(1, 3001):\n",
    "    a = torch.sigmoid(z)\n",
    "\n",
    "    util   = torch.mean((a * R).sum(dim=1) / k_rec)\n",
    "    L_util = -util\n",
    "\n",
    "    row_sums = a.sum(dim=1)\n",
    "    L_card   = torch.mean((row_sums - k_rec)**2)\n",
    "\n",
    "    col_sums = a.sum(dim=0)\n",
    "    L_prod   = torch.mean(torch.relu(prod_min - col_sums)**2)\n",
    "\n",
    "    L_bin  = torch.mean(a * (1 - a))\n",
    "\n",
    "    loss = lambda_util * L_util + lambda_card * L_card + lambda_prod * L_prod + L_bin\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"Epoch {epoch:5d} — loss: {loss.item():.4f}, util: {-L_util.item():.2f}, card: {L_card.item():.4f}, prod: {L_prod.item():.4f}, bin: {L_bin.item():.4f}\")\n",
    "\n",
    "    opt.step()\n",
    "    scheduler.step(loss.item())\n",
    "\n",
    "A_cont = torch.sigmoid(z).detach().cpu().numpy()\n",
    "# projection to binary\n",
    "A_opt = np.zeros_like(A_cont)\n",
    "for i, row in enumerate(A_cont):\n",
    "    A_opt[i, row.argsort()[-k_rec:]] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "20598bb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05779154, 0.05771403, 0.05765368, 0.05763114, 0.05748032,\n",
       "       0.05746737, 0.05745109, 0.0573871 , 0.05737695, 0.05736849,\n",
       "       0.05735333, 0.0573409 , 0.05724973, 0.05698105, 0.0569311 ,\n",
       "       0.05686256, 0.05677284, 0.05677127, 0.05671323, 0.0565905 ,\n",
       "       0.05654738, 0.05640062, 0.0563875 , 0.05636477, 0.05631623,\n",
       "       0.05626244, 0.0561929 , 0.05607791, 0.0560635 , 0.05598119,\n",
       "       0.055796  , 0.05579099, 0.05566983, 0.05551579, 0.05549545,\n",
       "       0.05544922, 0.05526333, 0.05515146, 0.05514026, 0.05512378,\n",
       "       0.0551208 , 0.05511221, 0.05501353, 0.05500968, 0.0549847 ,\n",
       "       0.0549709 , 0.05496861, 0.05494679, 0.05487379, 0.05481775,\n",
       "       0.05479058, 0.05460422, 0.05456612, 0.05438355, 0.05427985,\n",
       "       0.0542602 , 0.05414438, 0.05396806, 0.0538949 , 0.05344047,\n",
       "       0.05329363, 0.05327445, 0.05327269, 0.05324512, 0.05314672,\n",
       "       0.05307278, 0.0529814 , 0.05295432, 0.05279669, 0.05271975,\n",
       "       0.05261503, 0.05256554, 0.05239647, 0.05221638, 0.05215552,\n",
       "       0.05204444, 0.05185011, 0.05181617, 0.05175835, 0.05175507,\n",
       "       0.05171016, 0.05165974, 0.05161672, 0.05146614, 0.05146449,\n",
       "       0.05144338, 0.05123938, 0.05123462, 0.05123105, 0.05121798,\n",
       "       0.05105206, 0.05098351, 0.05088179, 0.05073218, 0.05070573,\n",
       "       0.05069255, 0.05065148, 0.05059477, 0.05058146, 0.05054818,\n",
       "       0.05049826, 0.05049675, 0.05030641, 0.05024936, 0.05019604,\n",
       "       0.05004277, 0.05003623, 0.05001414, 0.04984267, 0.04967172,\n",
       "       0.04964653, 0.04958618, 0.04952802, 0.04951746, 0.04943068,\n",
       "       0.04927364, 0.04907998, 0.0490788 , 0.04876338, 0.04874351,\n",
       "       0.04872711, 0.04852556, 0.04849245, 0.04842557, 0.04841946,\n",
       "       0.04820363, 0.04789652, 0.04786145, 0.04785348, 0.04776433,\n",
       "       0.047723  , 0.04762202, 0.04726925, 0.04704781, 0.04699937,\n",
       "       0.04692208, 0.04688252, 0.04673687, 0.04673035, 0.04668252,\n",
       "       0.04640101, 0.04632573, 0.04627066, 0.0461515 , 0.04579562,\n",
       "       0.04553593, 0.04548968, 0.04545632, 0.04539191, 0.04535106,\n",
       "       0.04517056, 0.04502737, 0.04500294, 0.04496462, 0.04495121,\n",
       "       0.044863  , 0.04482797, 0.04481931, 0.0446752 , 0.04465057,\n",
       "       0.04460389, 0.04460059, 0.04457141, 0.04454938, 0.04453877,\n",
       "       0.0443855 , 0.04434566, 0.0441755 , 0.0441749 , 0.04403119,\n",
       "       0.04401981, 0.04390153, 0.04386245, 0.04357236, 0.04348962,\n",
       "       0.04341037, 0.04335541, 0.04328381, 0.04316894, 0.04304557,\n",
       "       0.04273591, 0.04273373, 0.04270129, 0.04264156, 0.04260909,\n",
       "       0.0425135 , 0.0424351 , 0.04243381, 0.04237299, 0.04233916,\n",
       "       0.04221317, 0.04218304, 0.04199077, 0.0419712 , 0.04191751,\n",
       "       0.04191623, 0.04178182, 0.04160218, 0.04142548, 0.04131938],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(A_cont[0])[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "2bee0b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9046699930702641)"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.sum(A_opt * rel_mat, axis=1) / k_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "12d805dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.023157894478032476)"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_correlation(mean_allocations, A_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "0f6607e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.,  9.,  4.,  8., 13., 13., 10.,  6.,  6., 12., 11., 11., 21.,\n",
       "        4.,  7.,  8.,  6.,  8., 16.,  5., 15., 13., 20.,  9., 12., 10.,\n",
       "        6.,  2., 11.,  9., 14., 17., 21., 21., 10.,  8., 11.,  5.,  9.,\n",
       "       12.,  9.,  9., 14., 12., 13.,  7.,  9.,  3.,  8.,  6.,  9., 14.,\n",
       "        8., 12.,  9., 15.,  7., 19., 10.,  5., 14.,  9., 10.,  9., 12.,\n",
       "       15.,  9., 11., 10.,  9., 13., 10.,  6., 10.,  7., 10.,  6., 10.,\n",
       "       14.,  6.,  9., 12., 13.,  4.,  9.,  7.,  6.,  9.,  9.,  8., 14.,\n",
       "        9.,  9., 10.,  5., 16., 11.,  8., 10.,  7.,  9., 18., 12.,  4.,\n",
       "       11., 10., 12.,  9.,  4., 18., 14., 18., 17., 16.,  8.,  8., 15.,\n",
       "        7., 11.,  8., 10., 12.,  7.,  6.,  8., 12.,  3., 10.,  4.,  9.,\n",
       "        9., 11., 10., 14.,  4.,  9., 10., 12., 10., 10.,  6., 13.,  9.,\n",
       "       11.,  3.,  7.,  3.,  9.,  8., 14.,  6., 14.,  9.,  9., 10., 17.,\n",
       "       18., 11.,  5.,  9.,  9., 10., 11., 12., 16., 14., 14.,  1.,  5.,\n",
       "       13., 12.,  5., 11., 10.,  8., 12., 16., 13.,  6., 13.,  7., 11.,\n",
       "       11., 12.,  4.,  2.,  4., 14.,  7., 13.,  6.,  7., 11.,  8., 16.,\n",
       "        6.,  8.,  9., 13., 15.], dtype=float32)"
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_opt.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "9197ace8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 1] Epoch  500 | Loss: 625.000 | Util: 0.000 | Violations: 200\n",
      "[Stage 1] Epoch 1000 | Loss: 625.000 | Util: 0.000 | Violations: 200\n",
      "[Stage 2] Epoch 1500 | Loss: 4710.328 | Util: 0.000 | Violations: 200 | Avg Shortfall: 5.000\n",
      "[Stage 2] Epoch 2000 | Loss: 4710.328 | Util: 0.000 | Violations: 200 | Avg Shortfall: 5.000\n",
      "[Stage 2] Epoch 2500 | Loss: 4710.328 | Util: 0.000 | Violations: 200 | Avg Shortfall: 5.000\n",
      "[Stage 2] Epoch 3000 | Loss: 4710.328 | Util: 0.000 | Violations: 200 | Avg Shortfall: 5.000\n",
      "[Stage 2] Epoch 3500 | Loss: 4710.328 | Util: 0.000 | Violations: 200 | Avg Shortfall: 5.000\n",
      "[Stage 2] Epoch 4000 | Loss: 4710.328 | Util: 0.000 | Violations: 200 | Avg Shortfall: 5.000\n",
      "Final violations after projection: 5\n"
     ]
    }
   ],
   "source": [
    "n, m = rel_mat.shape  # ensure rel_matrix is defined\n",
    "R = torch.tensor(rel_mat, dtype=torch.float32, device=\"cpu\")\n",
    "\n",
    "z = (torch.randn(n, m, device=\"cpu\") * 0.01 + 0.1).requires_grad_(True)\n",
    "log_lambda_prod = torch.zeros(m, device=\"cpu\", requires_grad=False)\n",
    "\n",
    "opt = torch.optim.Adam([z], lr=0.05)\n",
    "\n",
    "\n",
    "# STAGE 1: Allow the model to find a meaningful initial solution (no lambdas initially)\n",
    "for epoch in range(1, 1001):\n",
    "    tau = max(0.05, 0.995 ** epoch)\n",
    "    a = torch.sigmoid(z / tau)\n",
    "\n",
    "    util = torch.mean((a * R).sum(dim=1) / k_rec)\n",
    "    L_util = -util\n",
    "\n",
    "    row_sums = a.sum(dim=1)\n",
    "    L_card = ((row_sums - k_rec)**2).mean()\n",
    "\n",
    "    col_sums = a.sum(dim=0)\n",
    "    prod_shortfall = torch.relu(prod_min - col_sums)\n",
    "    L_prod = (prod_shortfall**2).mean()  # soft quadratic penalty, no lambda yet\n",
    "\n",
    "    loss = L_util + 5.0 * L_card + 5.0 * L_prod  # balanced weights, moderate penalties\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        violated_count = (col_sums.detach().cpu().numpy() < prod_min).sum()\n",
    "        print(f\"[Stage 1] Epoch {epoch:4d} | Loss: {loss.item():.3f} | \"\n",
    "              f\"Util: {-L_util.item():.3f} | Violations: {violated_count}\")\n",
    "\n",
    "# STAGE 2: Now introduce controlled dual updates to enforce constraints strictly\n",
    "for epoch in range(1001, 4001):\n",
    "    tau = max(0.01, 0.995 ** epoch)\n",
    "    a = torch.sigmoid(z / tau)\n",
    "\n",
    "    util = torch.mean((a * R).sum(dim=1) / k_rec)\n",
    "    L_util = -util\n",
    "\n",
    "    row_sums = a.sum(dim=1)\n",
    "    L_card = ((row_sums - k_rec)**2).mean()\n",
    "\n",
    "    col_sums = a.sum(dim=0)\n",
    "    prod_shortfall = torch.relu(prod_min - col_sums)\n",
    "\n",
    "    lambda_prod = torch.exp(log_lambda_prod)\n",
    "    L_prod = (lambda_prod * (prod_shortfall**2)).mean()\n",
    "\n",
    "    loss = L_util + 10.0 * L_card + L_prod  # slightly stronger constraints now\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        log_lambda_prod += 0.01 * prod_shortfall  # controlled dual updates\n",
    "        log_lambda_prod.clamp_(min=-2, max=5)\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        violated_count = (col_sums.detach().cpu().numpy() < prod_min).sum()\n",
    "        avg_shortfall = prod_shortfall.mean().item()\n",
    "        print(f\"[Stage 2] Epoch {epoch:4d} | Loss: {loss.item():.3f} | \"\n",
    "              f\"Util: {-L_util.item():.3f} | Violations: {violated_count} | Avg Shortfall: {avg_shortfall:.3f}\")\n",
    "\n",
    "# Final binary projection\n",
    "A_cont = torch.sigmoid(z).detach().cpu().numpy()\n",
    "A_opt = np.zeros_like(A_cont)\n",
    "for i, row in enumerate(A_cont):\n",
    "    A_opt[i, row.argsort()[-k_rec:]] = 1\n",
    "\n",
    "producer_counts = A_opt.sum(axis=0)\n",
    "violations = (producer_counts < prod_min).sum()\n",
    "print(f\"Final violations after projection: {violations}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "044ba255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.8925312649793051)"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.sum(A_opt * rel_mat, axis=1) / k_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "c8ca5073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.,  5., 12.,  9., 13.,  8., 12.,  7., 11.,  6.,  9., 16., 13.,\n",
       "        9.,  8.,  4., 12.,  8.,  9., 12.,  9., 10., 11., 13., 12.,  7.,\n",
       "       11., 10., 10.,  9., 12., 11.,  9., 11., 10., 11., 11., 13.,  7.,\n",
       "        5.,  7.,  8., 14.,  6.,  3., 16., 11., 12.,  8., 13., 10., 13.,\n",
       "        7.,  8.,  6.,  6., 15., 11., 13., 11., 13., 13.,  4.,  9., 16.,\n",
       "       14., 13., 11.,  8., 12., 10., 11., 11., 10.,  9.,  7., 18., 10.,\n",
       "        9., 10., 15., 13.,  8., 11.,  9., 11.,  7., 12., 16., 11., 11.,\n",
       "       10., 13.,  7., 10., 14., 12.,  8., 10., 10.,  6., 14., 15.,  6.,\n",
       "       15., 17.,  6., 18., 12.,  9., 13., 13., 11., 10., 10., 15.,  8.,\n",
       "       10., 11.,  8., 14.,  9., 11.,  7.,  9., 13., 11., 11.,  5.,  6.,\n",
       "        7.,  6.,  8.,  8.,  9.,  9., 10.,  4., 11., 13., 11.,  6.,  9.,\n",
       "        6.,  6.,  9.,  7.,  9.,  7., 10.,  6., 17., 11.,  3.,  7., 19.,\n",
       "        7., 10.,  9., 11., 10.,  6.,  9., 11.,  6.,  9., 11., 13.,  9.,\n",
       "        8.,  8.,  9.,  5., 11., 11., 15.,  7.,  9., 12., 12., 10.,  7.,\n",
       "       14.,  8.,  6.,  8.,  7.,  7.,  7., 12., 14.,  7., 11., 15., 11.,\n",
       "       13., 10., 11., 11.,  6.], dtype=float32)"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_opt.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "fef373e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9466594812127047)"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.sum(mean_allocations * rel_mat, axis=1) / k_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "83a7be40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.88454426 0.82073556 0.92958885 0.95041268 0.90130418 0.95360957\n",
      " 0.90437249 0.90215369 0.84169271 0.81099343 0.74843775 0.96578593\n",
      " 0.94725356 0.95360391 0.89828264 0.8910981  0.78405916 0.89805986\n",
      " 0.94508075 0.97740006 0.79811105 0.87104322 0.98142324 0.85312095\n",
      " 0.8231652  0.91341474 0.89999115 0.8845513  0.9452226  0.85785696\n",
      " 0.91790804 0.97983441 0.91876659 0.89139027 0.89914497 0.8987039\n",
      " 0.81594811 0.89713778 0.82567525 0.91429423 0.9268033  0.91292709\n",
      " 0.90771376 0.97021217 0.92682126 0.94819362 0.8867817  0.87495038\n",
      " 0.93166013 0.7806246  0.73209766 0.78123458 0.92761685 0.86109238\n",
      " 0.96749913 0.79566761 0.89570176 0.93509685 0.9707759  0.72826213\n",
      " 0.84841753 0.72617826 0.92275587 0.95187544 0.91133795 0.87837534\n",
      " 0.89899835 0.80371097 0.71420634 0.93934083 0.80872552 0.78192086\n",
      " 0.92679767 0.93672771 0.96834136 0.74314832 0.73710272 0.84708724\n",
      " 0.88626303 0.84994237 0.88079671 0.79758221 0.81891578 0.87057058\n",
      " 0.97260963 0.97045069 0.7622217  0.96137044 0.73829369 0.96344746\n",
      " 0.88973921 0.82469258 0.58360167 0.87900636 0.76796995 0.75605182\n",
      " 0.98307    0.83221798 0.91599639 0.85516276 0.75216333 0.94905136\n",
      " 0.89920573 0.9679959  0.87263646 0.95255206 0.90563799 0.89101095\n",
      " 0.79797739 0.94706276 0.82360574 0.90061584 0.96498923 0.92985347\n",
      " 0.94607372 0.84307783 0.955918   0.67337864 0.93319916 0.85607468\n",
      " 0.9573425  0.9240877  0.92889848 0.98288104 0.95401157 0.93610961\n",
      " 0.88040985 0.85905188 0.72206079 0.95303008 0.96348601 0.87570441\n",
      " 0.97235428 0.64599618 0.52923471 0.94371783 0.98973409 0.74003518\n",
      " 0.93398385 0.87266274 0.90278946 0.67589982 0.85693599 0.9796301\n",
      " 0.85868077 0.93148637 0.92871379 0.895363   0.8500149  0.90239015\n",
      " 0.95411202 0.61314558 0.8673071  0.88870579 0.82210811 0.95265392\n",
      " 0.90248443 0.94544659 0.85347737 0.90759716 0.84584031 0.736178\n",
      " 0.8994435  0.89422202 0.91600939 0.77311299 0.94977742 0.91987507\n",
      " 0.90829731 0.92432204 0.85570909 0.89708291 0.78007659 0.9050306\n",
      " 0.90513737 0.83419953 0.81794808 0.92697442 0.8188353  0.92701695\n",
      " 0.98203026 0.80019731 0.83259549 0.77375549 0.91181075 0.88339291\n",
      " 0.96273598 0.95451481 0.87918476 0.87964963 0.8700256  0.94111865\n",
      " 0.90035722 0.92834275 0.96181777 0.83687859 0.89369618 0.89750249\n",
      " 0.9639097  0.80669042]\n",
      "[  5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.\n",
      "   5.   5.   5.  27.  14.   5.   5.   5.   5.   5.   5.   5.   5.   5.\n",
      " 116.   5. 115.   5.   5.  56. 139.   5.   5.   5.   5.   5.   5.   5.\n",
      "   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   6.\n",
      "   5.   5.   5.   5.   5.   5.   5.   5.   5. 127.   5.  26.   5.   5.\n",
      "   5.   5.  14.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.\n",
      "   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5. 154.   5.   5.\n",
      "   5.   5.   5.  30.   5.   5.   5.   5.   5.   5.   5.  10.  13.   5.\n",
      "   5.  45.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.\n",
      "   5.   5.   5.   5.   5.  13.   5.   5.   5.   5.   5.   5.   5.   5.\n",
      "   5.   5.   5.   5.   5.   5.   5.   8.   5.  28.   5.   5.   5.   5.\n",
      "   5.  48.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.\n",
      "   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.\n",
      "   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.\n",
      " 111.   5.   5.   5.]\n",
      "[10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      " 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      " 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      " 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      " 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      " 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      " 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      " 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      " 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      " 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      " 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      " 10. 10.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# for each rel_mat row, take top-k based on the allocation\n",
    "sorted_indices = np.argsort(mean_allocations, axis=1)[::-1][:, :k_rec]\n",
    "print(np.mean(rel_mat[np.arange(rel_mat.shape[0])[:, None], sorted_indices], axis=1))\n",
    "print(np.sum(mean_allocations, axis=0))\n",
    "print(np.sum(mean_allocations, axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "b0aef12a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.3194736703774324)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute correlation between the two allocations\n",
    "\n",
    "\n",
    "corr = compute_correlation(new_alls, mean_allocations)\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "789c6f0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.])"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws = torch.randn(n, m, device=\"cpu\")\n",
    "gumbel = F.gumbel_softmax(ws, tau=tau, hard=True, dim=-1)\n",
    "\n",
    "gumbel[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
