{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80fc4297",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import json\n",
    "import math\n",
    "import cvxpy as cp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from src.problems.utils import sample_data_for_group\n",
    "from src.problems.problems import compute_producer_optimal_solution, _compute_consumer_optimal_solution_cvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32189296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_correlation(a, b):\n",
    "    a = a.flatten()\n",
    "    b = b.flatten()\n",
    "\n",
    "    a_mean = np.mean(a)\n",
    "    b_mean = np.mean(b)\n",
    "\n",
    "    numerator = np.sum((a - a_mean) * (b - b_mean))\n",
    "    denominator = np.sqrt(np.sum((a - a_mean) ** 2) * np.sum((b - b_mean) ** 2))\n",
    "\n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f67f0247",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH_ROOT = Path(\"../../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01ac3b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "with open(DATA_PATH_ROOT / \"amazon_predictions.npy\", \"rb\") as f:\n",
    "    REL_MATRIX = np.load(f)\n",
    "\n",
    "with open(DATA_PATH_ROOT / \"amazon_user_groups.json\", \"r\") as f:\n",
    "    GROUPS_MAP = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9034f784",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CONSUMERS = 200\n",
    "N_PRODUCERS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56e4e579",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_matrix, consumer_ids, group_assignments = sample_data_for_group(\n",
    "    n_consumers=N_CONSUMERS,\n",
    "    n_producers=N_PRODUCERS,\n",
    "    groups_map=GROUPS_MAP,\n",
    "    group_key=\"usage_group\",\n",
    "    data=REL_MATRIX,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a40aa00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9.999999999999998,\n",
       " array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], shape=(200, 200)))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_producer_optimal_solution(\n",
    "    rel_matrix=sampled_matrix,\n",
    "    k_rec=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4be50f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cvar_allocations = _compute_consumer_optimal_solution_cvar(\n",
    "    rel_matrix=sampled_matrix,\n",
    "    k_rec=10,\n",
    "    producer_max_min_utility=10,\n",
    "    group_assignments=group_assignments,\n",
    "    gamma=0.5,\n",
    "    alpha=0.5,\n",
    "    solver=cp.SCIP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6e85f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvar_allocations = cvar_allocations.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "828ae27f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9485004278846448)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.sum(cvar_allocations * sampled_matrix, axis=1)) / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "31e22b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerSelector(nn.Module):\n",
    "    def __init__(self, n_users, hidden_dim, n_items):\n",
    "        super().__init__()\n",
    "        # first “layer”: maps each user to a hidden representation\n",
    "        self.z1 = nn.Parameter(torch.empty(n_users, hidden_dim))\n",
    "        self.h1 = nn.Parameter(torch.empty(hidden_dim, hidden_dim))\n",
    "        self.z2 = nn.Parameter(torch.empty(hidden_dim, n_items))\n",
    "        self.b1 = nn.Parameter(torch.zeros(hidden_dim))\n",
    "        self.b2 = nn.Parameter(torch.zeros(n_items))\n",
    "        self.rho = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.z1)\n",
    "        nn.init.xavier_uniform_(self.h1)\n",
    "        nn.init.xavier_uniform_(self.z2)\n",
    "        nn.init.zeros_(self.b1)\n",
    "        nn.init.zeros_(self.b2)\n",
    "        nn.init.zeros_(self.rho)\n",
    "\n",
    "    def forward(self):\n",
    "        x = torch.tanh(self.z1 @ self.h1 + self.b1)\n",
    "        logits = x @ self.z2 + self.b2\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66559b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dominykas.seputis/github/msc-thesis/.venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     1 — loss: 8097.7524, util: 0.00, card: 8097.6899, prod: 0.0000, bin: 0.0623,  rho: -0.0010, tau: 0.9990\n",
      "[Epoch   500] grad_norm: 0.7761\n",
      "Epoch   500 — loss: 0.0611, util: 0.04, card: 0.0187, prod: 0.0000, bin: 0.0031,  rho: -0.0060, tau: 0.6064\n",
      "[Epoch  1000] grad_norm: 0.7540\n",
      "Epoch  1000 — loss: 0.0462, util: 0.03, card: 0.0154, prod: 0.0000, bin: 0.0015,  rho: -0.0060, tau: 0.3677\n",
      "[Epoch  1500] grad_norm: 0.5528\n",
      "Epoch  1500 — loss: 0.0417, util: 0.03, card: 0.0101, prod: 0.0000, bin: 0.0011,  rho: -0.0060, tau: 0.2230\n",
      "[Epoch  2000] grad_norm: 0.7967\n",
      "Epoch  2000 — loss: 0.0404, util: 0.03, card: 0.0101, prod: 0.0000, bin: 0.0012,  rho: -0.0060, tau: 0.1352\n",
      "[Epoch  2500] grad_norm: 0.6067\n",
      "Epoch  2500 — loss: 0.0387, util: 0.03, card: 0.0077, prod: 0.0000, bin: 0.0014,  rho: -0.0060, tau: 0.0820\n",
      "[Epoch  3000] grad_norm: 1.1865\n",
      "Epoch  3000 — loss: 0.0377, util: 0.03, card: 0.0088, prod: 0.0000, bin: 0.0019,  rho: -0.0060, tau: 0.0497\n",
      "[Epoch  3500] grad_norm: 1.7302\n",
      "Epoch  3500 — loss: 0.0338, util: 0.02, card: 0.0067, prod: 0.0000, bin: 0.0023,  rho: -0.0060, tau: 0.0301\n",
      "[Epoch  4000] grad_norm: 1.3250\n",
      "Epoch  4000 — loss: 0.0265, util: 0.02, card: 0.0076, prod: 0.0000, bin: 0.0017,  rho: -0.0060, tau: 0.0183\n",
      "[Epoch  4500] grad_norm: 0.7412\n",
      "Epoch  4500 — loss: 0.0223, util: 0.02, card: 0.0048, prod: 0.0001, bin: 0.0008,  rho: -0.0060, tau: 0.0111\n",
      "[Epoch  5000] grad_norm: 1.0917\n",
      "Epoch  5000 — loss: 0.0208, util: 0.02, card: 0.0050, prod: 0.0000, bin: 0.0005,  rho: -0.0060, tau: 0.0100\n",
      "[Epoch  5500] grad_norm: 0.1912\n",
      "Epoch  5500 — loss: 0.0195, util: 0.01, card: 0.0050, prod: 0.0000, bin: 0.0003,  rho: -0.0060, tau: 0.0100\n",
      "[Epoch  6000] grad_norm: 0.2620\n",
      "Epoch  6000 — loss: 0.0191, util: 0.01, card: 0.0056, prod: 0.0000, bin: 0.0002,  rho: -0.0060, tau: 0.0100\n",
      "[Epoch  6500] grad_norm: 0.3171\n",
      "Epoch  6500 — loss: 0.0188, util: 0.01, card: 0.0066, prod: 0.0000, bin: 0.0002,  rho: -0.0060, tau: 0.0100\n",
      "[Epoch  7000] grad_norm: 0.1881\n",
      "Epoch  7000 — loss: 0.0182, util: 0.01, card: 0.0055, prod: 0.0000, bin: 0.0002,  rho: -0.0060, tau: 0.0100\n",
      "[Epoch  7500] grad_norm: 0.0806\n",
      "Epoch  7500 — loss: 0.0178, util: 0.01, card: 0.0050, prod: 0.0000, bin: 0.0002,  rho: -0.0060, tau: 0.0100\n",
      "[Epoch  8000] grad_norm: 0.9577\n",
      "Epoch  8000 — loss: 0.0180, util: 0.01, card: 0.0051, prod: 0.0000, bin: 0.0002,  rho: -0.0060, tau: 0.0100\n",
      "[Epoch  8500] grad_norm: 0.2083\n",
      "Epoch  8500 — loss: 0.0177, util: 0.01, card: 0.0055, prod: 0.0000, bin: 0.0002,  rho: -0.0060, tau: 0.0100\n",
      "[Epoch  9000] grad_norm: 0.1731\n",
      "Epoch  9000 — loss: 0.0174, util: 0.01, card: 0.0053, prod: 0.0000, bin: 0.0002,  rho: -0.0060, tau: 0.0100\n",
      "[Epoch  9500] grad_norm: 0.1869\n",
      "Epoch  9500 — loss: 0.0172, util: 0.01, card: 0.0054, prod: 0.0000, bin: 0.0001,  rho: -0.0060, tau: 0.0100\n",
      "[Epoch 10000] grad_norm: 0.1756\n",
      "Epoch 10000 — loss: 0.0169, util: 0.01, card: 0.0052, prod: 0.0000, bin: 0.0001,  rho: -0.0060, tau: 0.0100\n",
      "[Epoch 10500] grad_norm: 0.1273\n",
      "Epoch 10500 — loss: 0.0166, util: 0.01, card: 0.0054, prod: 0.0000, bin: 0.0001,  rho: -0.0060, tau: 0.0100\n",
      "[Epoch 11000] grad_norm: 0.1197\n",
      "Epoch 11000 — loss: 0.0165, util: 0.01, card: 0.0051, prod: 0.0000, bin: 0.0002,  rho: -0.0060, tau: 0.0100\n",
      "[Epoch 11500] grad_norm: 0.1319\n",
      "Epoch 11500 — loss: 0.0163, util: 0.01, card: 0.0051, prod: 0.0000, bin: 0.0001,  rho: -0.0060, tau: 0.0100\n",
      "[Epoch 12000] grad_norm: 0.1203\n",
      "Epoch 12000 — loss: 0.0160, util: 0.01, card: 0.0051, prod: 0.0000, bin: 0.0001,  rho: -0.0060, tau: 0.0100\n",
      "[Epoch 12500] grad_norm: 0.1408\n",
      "Epoch 12500 — loss: 0.0159, util: 0.01, card: 0.0054, prod: 0.0000, bin: 0.0001,  rho: -0.0060, tau: 0.0100\n",
      "[Epoch 13000] grad_norm: 0.0975\n",
      "Epoch 13000 — loss: 0.0157, util: 0.01, card: 0.0051, prod: 0.0000, bin: 0.0001,  rho: -0.0060, tau: 0.0100\n",
      "[Epoch 13500] grad_norm: 0.0949\n",
      "Epoch 13500 — loss: 0.0156, util: 0.01, card: 0.0052, prod: 0.0000, bin: 0.0001,  rho: -0.0060, tau: 0.0100\n",
      "[Epoch 14000] grad_norm: 0.1052\n",
      "Epoch 14000 — loss: 0.0156, util: 0.01, card: 0.0051, prod: 0.0000, bin: 0.0001,  rho: -0.0060, tau: 0.0100\n",
      "[Epoch 14500] grad_norm: 0.9403\n",
      "Epoch 14500 — loss: 0.0156, util: 0.01, card: 0.0050, prod: 0.0000, bin: 0.0001,  rho: -0.0060, tau: 0.0100\n",
      "[Epoch 15000] grad_norm: 0.1172\n",
      "Epoch 15000 — loss: 0.0155, util: 0.01, card: 0.0052, prod: 0.0000, bin: 0.0001,  rho: -0.0060, tau: 0.0100\n",
      "[Epoch 15500] grad_norm: 0.0632\n",
      "Epoch 15500 — loss: 0.0153, util: 0.01, card: 0.0050, prod: 0.0000, bin: 0.0001,  rho: -0.0060, tau: 0.0100\n",
      "[Epoch 16000] grad_norm: 0.1189\n",
      "Epoch 16000 — loss: 0.0153, util: 0.01, card: 0.0052, prod: 0.0000, bin: 0.0001,  rho: -0.0060, tau: 0.0100\n",
      "[Epoch 16500] grad_norm: 0.0755\n",
      "Epoch 16500 — loss: 0.0152, util: 0.01, card: 0.0050, prod: 0.0000, bin: 0.0001,  rho: -0.0060, tau: 0.0100\n",
      "[Epoch 17000] grad_norm: 0.1163\n",
      "Epoch 17000 — loss: 0.0151, util: 0.01, card: 0.0053, prod: 0.0000, bin: 0.0001,  rho: -0.0060, tau: 0.0100\n",
      "[Epoch 17500] grad_norm: 0.0870\n",
      "Epoch 17500 — loss: 0.0149, util: 0.01, card: 0.0050, prod: 0.0000, bin: 0.0001,  rho: -0.0060, tau: 0.0100\n",
      "[Epoch 18000] grad_norm: 0.1079\n",
      "Epoch 18000 — loss: 0.0149, util: 0.01, card: 0.0052, prod: 0.0000, bin: 0.0001,  rho: -0.0060, tau: 0.0100\n",
      "[Epoch 18500] grad_norm: 0.1017\n",
      "Epoch 18500 — loss: 0.0148, util: 0.01, card: 0.0051, prod: 0.0000, bin: 0.0001,  rho: -0.0060, tau: 0.0100\n",
      "[Epoch 19000] grad_norm: 0.0922\n",
      "Epoch 19000 — loss: 0.0146, util: 0.01, card: 0.0052, prod: 0.0000, bin: 0.0001,  rho: -0.0060, tau: 0.0100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[161], line 56\u001b[0m\n\u001b[1;32m     52\u001b[0m allocations \u001b[38;5;241m=\u001b[39m (a \u001b[38;5;241m*\u001b[39m R)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m#L_util = mean_util(allocations)\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m L_util \u001b[38;5;241m=\u001b[39m \u001b[43mcvar_util\u001b[49m\u001b[43m(\u001b[49m\u001b[43mR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallocations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup_assignments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_rec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrho\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m row_sums \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     59\u001b[0m L_card   \u001b[38;5;241m=\u001b[39m (row_sums \u001b[38;5;241m-\u001b[39m k_rec)\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n",
      "Cell \u001b[0;32mIn[161], line 23\u001b[0m, in \u001b[0;36mcvar_util\u001b[0;34m(rel_matrix, allocations, group_assignments, k_rec, rho, alpha)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcvar_util\u001b[39m(rel_matrix, allocations, group_assignments, k_rec, rho, alpha):\n\u001b[0;32m---> 23\u001b[0m     greedy_allocations \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrel_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m[:, \u001b[38;5;241m-\u001b[39mk_rec:]\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     25\u001b[0m     unique_groups, group_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(group_assignments, return_inverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     26\u001b[0m     num_groups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(unique_groups)\n",
      "File \u001b[0;32m~/github/msc-thesis/.venv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:1122\u001b[0m, in \u001b[0;36msort\u001b[0;34m(a, axis, kind, order, stable)\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1121\u001b[0m     a \u001b[38;5;241m=\u001b[39m asanyarray(a)\u001b[38;5;241m.\u001b[39mcopy(order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mK\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1122\u001b[0m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "R = torch.tensor(sampled_matrix)\n",
    "n, m = R.shape\n",
    "hidden_dim = 200\n",
    "model = TwoLayerSelector(n, hidden_dim, m)\n",
    "\n",
    "initial_tau = 1.0\n",
    "final_tau = 0.01\n",
    "anneal_rate = 0.999\n",
    "max_norm = 4\n",
    "opt = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(opt, patience=100, factor=0.5, verbose=True)\n",
    "\n",
    "# your constants…\n",
    "lambda_util, lambda_card, lambda_prod, lambda_bin = 1, 1, 1, 1\n",
    "k_rec, decay = 10, 1\n",
    "prod_min = torch.ones(m) * 5  # example\n",
    "\n",
    "def mean_util(allocations):\n",
    "    return -allocations.mean()\n",
    "\n",
    "\n",
    "def cvar_util(rel_matrix, allocations, group_assignments, k_rec, rho, alpha):\n",
    "    greedy_allocations = torch.tensor(np.sort(rel_matrix, axis=1)[:, -k_rec:].sum(axis=1), requires_grad=False)\n",
    "\n",
    "    unique_groups, group_indices = np.unique(group_assignments, return_inverse=True)\n",
    "    num_groups = len(unique_groups)\n",
    "    group_masks = [group_indices == i for i in range(num_groups)]\n",
    "    group_sizes = np.array([mask.sum() for mask in group_masks])\n",
    "\n",
    "    normalized_losses = []\n",
    "    for mask, size in zip(group_masks, group_sizes):\n",
    "        group_alloc = allocations[mask]\n",
    "        greedy_group_alloc = greedy_allocations[mask]\n",
    "\n",
    "        # compute loss for each group\n",
    "        normalized_loss = torch.sum(1 - (group_alloc / greedy_group_alloc)) / size\n",
    "        normalized_losses.append(normalized_loss)\n",
    "\n",
    "    rho_clamped = torch.clamp(rho, min=0)\n",
    "    cvar_objective = rho_clamped + (1 / ((1 - alpha) * num_groups)) * torch.sum(\n",
    "        torch.relu(torch.hstack(normalized_losses) - rho_clamped)\n",
    "    )\n",
    "\n",
    "    return cvar_objective\n",
    "\n",
    "\n",
    "tau = 1\n",
    "for epoch in range(1, 50001):\n",
    "    # initial tau 1, anneal to 0.01 with anneal_rate 0.999\n",
    "    tau = max(initial_tau * (anneal_rate ** epoch), final_tau)\n",
    "\n",
    "    a = torch.sigmoid(model() / tau)\n",
    "    allocations = (a * R).sum(dim=1)\n",
    "\n",
    "\n",
    "    #L_util = mean_util(allocations)\n",
    "    L_util = cvar_util(R, allocations, group_assignments, k_rec, model.rho, alpha=0.5)\n",
    "\n",
    "    row_sums = a.sum(dim=1)\n",
    "    L_card   = (row_sums - k_rec).pow(2).mean()\n",
    "\n",
    "    col_sums = a.sum(dim=0)\n",
    "    L_prod   = torch.relu(prod_min - col_sums).pow(2).mean()\n",
    "\n",
    "    L_bin  = (a * (1 - a)).pow(2).mean()\n",
    "\n",
    "    loss = (lambda_util\n",
    "                +  L_card\n",
    "                +  L_prod + L_bin)\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "    opt.step()\n",
    "    scheduler.step(loss)\n",
    "\n",
    "    # --- optional: track gradient norms ---\n",
    "    if epoch % 500 == 0:\n",
    "        total_norm = 0.0\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                total_norm += p.grad.data.norm(2).item()**2\n",
    "        total_norm = total_norm**0.5\n",
    "        print(f\"[Epoch {epoch:5d}] grad_norm: {total_norm:.4f}\")\n",
    "\n",
    "\n",
    "    if epoch % 500 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch:5d} — \"\n",
    "              f\"loss: {loss.item():.4f}, \"\n",
    "              f\"util: {L_util.item():.2f}, \"\n",
    "              f\"card: {L_card.item():.4f}, \"\n",
    "              f\"prod: {L_prod.item():.4f}, \"\n",
    "                f\"bin: {L_bin.item():.4f}, \",\n",
    "                f\"rho: {model.rho.item():.4f}, \"\n",
    "                f\"tau: {tau:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "f020e195",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.sigmoid(model.eval().forward() / tau)   # (n_users, n_items)\n",
    "\n",
    "out = logits.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "25724b49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5,   5,   5,   5,  94,   5,   5,   5,   5,   5,   5,   5,   5,\n",
       "         5,   5,   5,   5,   5,  41,   5,   5,   5,   5,   5,   5,   5,\n",
       "         5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,\n",
       "         5,   5,   5,   5,   5,   5,   5,   5,  55,   5,   5,   5,   5,\n",
       "         5,   5,   5,  26,   5,   5,   5,   5,   5,   5,  15,   5,   5,\n",
       "        63,   5,  16,   5,   5, 131,   5,   5,   5,   5,   5,   5,   5,\n",
       "         5,   5,   5,   5,   5,   5,   9,   5,   5,   5,   5,   5,   5,\n",
       "         5,   5,   5,   5,   5,   5,   5,   5,  66,   5,   5,   5,   5,\n",
       "         5,   5,   5,   6,   5,  12,   5,   5,   5,   5,   5,   5,   5,\n",
       "         5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,\n",
       "         5,   5,   5,   5, 157,   6,   5,   5,   5,   5,   5,   5,   5,\n",
       "         5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,\n",
       "        36,   5,   5,   5,   5,   5,   5,   5,  29,   5,   5,   5,   5,\n",
       "         5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,  11,   5,\n",
       "         5,   5, 111,   5,  80,   5,   5,   5,   5,   5,   5,   5, 136,\n",
       "         5,   5,   5,   5,   5])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvar_allocations.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "06d22039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5.0007796,   4.9985723,   4.997753 ,   4.9968405,  86.908195 ,\n",
       "         4.9970036,   4.995334 ,   5.0012746,   4.9959536,   4.997747 ,\n",
       "         4.9983277,   4.9979777,   5.0010533,   4.997963 ,   4.9956746,\n",
       "         4.9966035,   4.9965034,   5.000844 ,  20.389004 ,   5.0026903,\n",
       "         4.9959445,   4.9973497,   4.9975996,   4.9965816,   4.9973397,\n",
       "         5.0014334,   4.996321 ,   4.996391 ,   4.9949956,   4.996012 ,\n",
       "         4.9980893,   4.996876 ,   5.000072 ,   4.998816 ,   4.9917173,\n",
       "         4.997692 ,   4.9969907,   4.998713 ,   4.9963875,   4.998509 ,\n",
       "         4.9995584,   4.998403 ,   4.9971757,   4.999933 ,   4.996235 ,\n",
       "         4.9993587,   4.9965878,  75.481224 ,   4.9990354,   4.9963117,\n",
       "         4.9982295,   4.9954143,   4.9964104,   4.9984565,   4.997294 ,\n",
       "         8.421611 ,   4.99713  ,   4.99837  ,   5.001943 ,   4.996175 ,\n",
       "         4.994574 ,   4.997929 ,   4.999297 ,   4.9947515,   5.0008316,\n",
       "        13.766382 ,   4.9952035,   6.4774942,   4.99806  ,   4.9941792,\n",
       "       199.99988  ,   4.9950337,   4.998711 ,   4.9968724,   4.99704  ,\n",
       "         4.997132 ,   5.0002   ,   4.999204 ,   5.0020247,   4.997602 ,\n",
       "         4.999883 ,   4.99838  ,   4.9972506,   4.9975305,   5.003193 ,\n",
       "         4.997086 ,   4.9981008,   4.997253 ,   4.9962034,   4.998029 ,\n",
       "         4.993896 ,   4.995537 ,   4.9973145,   4.997247 ,   4.9977984,\n",
       "         5.000031 ,   4.9947653,   4.9969807,   4.9959927,  90.10627  ,\n",
       "         4.997312 ,   5.000245 ,   4.9975877,   4.9957414,   4.995396 ,\n",
       "         4.9994745,   4.9972253,   5.0003004,   4.9999976,  11.918783 ,\n",
       "         4.9947824,   5.000303 ,   4.998235 ,   4.9971247,   4.996929 ,\n",
       "         5.0017734,   4.998382 ,   4.9995155,   4.996121 ,   4.9936547,\n",
       "         4.997521 ,   4.996173 ,   4.9972367,   4.9961486,   4.9959116,\n",
       "         5.0077777,   4.998926 ,   4.9974246,   5.00228  ,   4.9924483,\n",
       "         5.002589 ,   5.000254 ,   4.9947643,   4.9999313, 200.       ,\n",
       "         5.213885 ,   4.9982595,   4.994595 ,   4.9984493,   5.000417 ,\n",
       "         4.9948406,   4.995887 ,   4.996939 ,   5.0005307,   4.995667 ,\n",
       "         4.999269 ,   4.998809 ,   4.998859 ,   4.9964375,   4.9979496,\n",
       "         4.9994025,   4.996166 ,   4.996753 ,   4.99549  ,   4.994578 ,\n",
       "         4.995656 ,  13.996672 ,   4.9944897,   4.99961  ,   4.9970183,\n",
       "         4.9955993,   4.9992714,   4.9949746,   4.9981127,   6.0381417,\n",
       "         5.003296 ,   4.9983754,   5.0013585,   4.996677 ,   4.9996505,\n",
       "         4.998747 ,   4.9963565,   4.9959326,   4.997235 ,   4.9967093,\n",
       "         5.018304 ,   5.1619234,   4.9969745,   4.9973283,   4.998972 ,\n",
       "         5.0665245,   4.997906 ,   4.9991837,   4.99948  , 141.69809  ,\n",
       "         4.997076 ,  13.587122 ,   4.997658 ,   4.9958725,   4.9969397,\n",
       "         4.9956465,   4.9963446,   4.998727 ,   4.998937 , 200.       ,\n",
       "         4.997133 ,   4.9946632,   4.999055 ,   4.9965663,   4.9978676],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "71b0e60a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5.,   5.,   5.,   5.,  85.,   5.,   5.,   5.,   5.,   5.,   5.,\n",
       "         5.,   5.,   5.,   5.,   5.,   5.,   5.,  19.,   5.,   5.,   5.,\n",
       "         5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,\n",
       "         5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,\n",
       "         5.,   5.,   5.,  74.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,\n",
       "         8.,   5.,   5.,   5.,   5.,   4.,   5.,   5.,   5.,   5.,  12.,\n",
       "         5.,   6.,   5.,   5., 200.,   5.,   5.,   5.,   5.,   5.,   5.,\n",
       "         4.,   5.,   5.,   5.,   4.,   5.,   5.,   5.,   5.,   5.,   5.,\n",
       "         5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,\n",
       "        90.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,  12.,\n",
       "         5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,\n",
       "         5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   4.,   5.,\n",
       "         5.,   5., 200.,   5.,   5.,   5.,   5.,   5.,   5.,   4.,   5.,\n",
       "         5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,\n",
       "         5.,   5.,  13.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   6.,\n",
       "         5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   4.,\n",
       "         5.,   5.,   5.,   5.,   5.,   5.,   5.,   5., 142.,   5.,  13.,\n",
       "         5.,   5.,   5.,   5.,   5.,   5.,   5., 200.,   5.,   5.,   5.,\n",
       "         5.,   5.], dtype=float32)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(out).sum(axis=0)  # check that each user has k_rec items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "5d937902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_std_between_groups(out, group_assignments):\n",
    "    unique_groups, group_indices = np.unique(group_assignments, return_inverse=True)\n",
    "    num_groups = len(unique_groups)\n",
    "    group_masks = [group_indices == i for i in range(num_groups)]\n",
    "    group_sizes = np.array([mask.sum() for mask in group_masks])\n",
    "\n",
    "    means = []\n",
    "    for mask, size in zip(group_masks, group_sizes):\n",
    "        group_alloc = out[mask]\n",
    "        mean = np.mean(group_alloc.sum(axis=0))\n",
    "        means.append(mean)\n",
    "\n",
    "    return np.mean(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "e8d34096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(3.1376325527596727), np.float64(3.1616680929488155))"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_std_between_groups(A_opt * sampled_matrix, group_assignments), compute_std_between_groups(cvar_allocations * sampled_matrix, group_assignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "59c9e29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_cont = model.eval().forward().detach().cpu().numpy()\n",
    "A_opt = np.zeros_like(A_cont)\n",
    "for i, row in enumerate(A_cont):\n",
    "    A_opt[i, row.argsort()[-k_rec:]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "4ed22395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5,   5,   5,   5,  94,   5,   5,   5,   5,   5,   5,   5,   5,\n",
       "         5,   5,   5,   5,   5,  41,   5,   5,   5,   5,   5,   5,   5,\n",
       "         5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,\n",
       "         5,   5,   5,   5,   5,   5,   5,   5,  55,   5,   5,   5,   5,\n",
       "         5,   5,   5,  26,   5,   5,   5,   5,   5,   5,  15,   5,   5,\n",
       "        63,   5,  16,   5,   5, 131,   5,   5,   5,   5,   5,   5,   5,\n",
       "         5,   5,   5,   5,   5,   5,   9,   5,   5,   5,   5,   5,   5,\n",
       "         5,   5,   5,   5,   5,   5,   5,   5,  66,   5,   5,   5,   5,\n",
       "         5,   5,   5,   6,   5,  12,   5,   5,   5,   5,   5,   5,   5,\n",
       "         5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,\n",
       "         5,   5,   5,   5, 157,   6,   5,   5,   5,   5,   5,   5,   5,\n",
       "         5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,\n",
       "        36,   5,   5,   5,   5,   5,   5,   5,  29,   5,   5,   5,   5,\n",
       "         5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,  11,   5,\n",
       "         5,   5, 111,   5,  80,   5,   5,   5,   5,   5,   5,   5, 136,\n",
       "         5,   5,   5,   5,   5])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvar_allocations.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "2e3d6bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5,   5,   5,   5,  86,   5,   5,   5,   5,   5,   5,   5,   5,\n",
       "         5,   5,   5,   5,   5,  19,   5,   5,   5,   5,   5,   5,   5,\n",
       "         5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,\n",
       "         5,   5,   5,   5,   5,   5,   5,   5,  74,   5,   5,   5,   5,\n",
       "         5,   5,   5,   8,   5,   5,   5,   5,   4,   5,   5,   5,   5,\n",
       "        12,   5,   6,   5,   5, 200,   5,   5,   5,   5,   5,   5,   4,\n",
       "         5,   5,   5,   4,   5,   5,   5,   5,   4,   5,   5,   5,   5,\n",
       "         5,   5,   5,   5,   5,   5,   5,   5,  90,   5,   5,   5,   5,\n",
       "         5,   5,   5,   5,   5,  12,   5,   5,   5,   5,   5,   5,   5,\n",
       "         5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,\n",
       "         4,   5,   5,   5, 200,   5,   5,   5,   6,   5,   5,   4,   5,\n",
       "         5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,\n",
       "        13,   5,   5,   5,   5,   5,   5,   5,   6,   5,   5,   5,   5,\n",
       "         5,   5,   5,   5,   5,   5,   4,   5,   5,   5,   5,   5,   5,\n",
       "         5,   5, 142,   5,  13,   5,   5,   5,   5,   5,   5,   5, 200,\n",
       "         5,   5,   5,   5,   5])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_opt.sum(axis=0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "6d538269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9415035986031873)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.sum(A_opt * sampled_matrix, axis=1) / k_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "f2212c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "       10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "       10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "       10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "       10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "       10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "       10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "       10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "       10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "       10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "       10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "       10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "       10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "       10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "       10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "       10., 10., 10., 10., 10.], dtype=float32)"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_opt.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "936472fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.7347368338939393)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_correlation(A_opt, mean_allocations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "514429df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dominykas.seputis/github/msc-thesis/.venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   500 — loss: -8.8511, util: 0.89, card: 0.0004, prod: 0.0000, bin: 0.0476\n",
      "Epoch  1000 — loss: -8.8768, util: 0.89, card: 0.0000, prod: 0.0000, bin: 0.0475\n",
      "Epoch  1500 — loss: -8.8769, util: 0.89, card: 0.0000, prod: 0.0000, bin: 0.0475\n",
      "Epoch  2000 — loss: -8.8771, util: 0.89, card: 0.0000, prod: 0.0000, bin: 0.0475\n",
      "Epoch  2500 — loss: -8.8773, util: 0.89, card: 0.0000, prod: 0.0000, bin: 0.0475\n",
      "Epoch  3000 — loss: -8.8776, util: 0.89, card: 0.0000, prod: 0.0000, bin: 0.0475\n"
     ]
    }
   ],
   "source": [
    "\n",
    "R = torch.tensor(rel_mat)\n",
    "n, m = R.shape\n",
    "z = torch.rand(n, m, requires_grad=True, device=\"cpu\")\n",
    "opt = torch.optim.Adam([z], lr=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, patience=100, factor=0.5, verbose=True)\n",
    "prod_min = 5\n",
    "\n",
    "\n",
    "lambda_util = 10\n",
    "lambda_card = 100\n",
    "lambda_prod = 10\n",
    "k_rec = 10\n",
    "decay = 1e-2\n",
    "\n",
    "for epoch in range(1, 3001):\n",
    "    a = torch.sigmoid(z)\n",
    "\n",
    "    util   = torch.mean((a * R).sum(dim=1) / k_rec)\n",
    "    L_util = -util\n",
    "\n",
    "    row_sums = a.sum(dim=1)\n",
    "    L_card   = torch.mean((row_sums - k_rec)**2)\n",
    "\n",
    "    col_sums = a.sum(dim=0)\n",
    "    L_prod   = torch.mean(torch.relu(prod_min - col_sums)**2)\n",
    "\n",
    "    L_bin  = torch.mean(a * (1 - a))\n",
    "\n",
    "    loss = lambda_util * L_util + lambda_card * L_card + lambda_prod * L_prod + L_bin\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"Epoch {epoch:5d} — loss: {loss.item():.4f}, util: {-L_util.item():.2f}, card: {L_card.item():.4f}, prod: {L_prod.item():.4f}, bin: {L_bin.item():.4f}\")\n",
    "\n",
    "    opt.step()\n",
    "    scheduler.step(loss.item())\n",
    "\n",
    "A_cont = torch.sigmoid(z).detach().cpu().numpy()\n",
    "# projection to binary\n",
    "A_opt = np.zeros_like(A_cont)\n",
    "for i, row in enumerate(A_cont):\n",
    "    A_opt[i, row.argsort()[-k_rec:]] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "20598bb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05779154, 0.05771403, 0.05765368, 0.05763114, 0.05748032,\n",
       "       0.05746737, 0.05745109, 0.0573871 , 0.05737695, 0.05736849,\n",
       "       0.05735333, 0.0573409 , 0.05724973, 0.05698105, 0.0569311 ,\n",
       "       0.05686256, 0.05677284, 0.05677127, 0.05671323, 0.0565905 ,\n",
       "       0.05654738, 0.05640062, 0.0563875 , 0.05636477, 0.05631623,\n",
       "       0.05626244, 0.0561929 , 0.05607791, 0.0560635 , 0.05598119,\n",
       "       0.055796  , 0.05579099, 0.05566983, 0.05551579, 0.05549545,\n",
       "       0.05544922, 0.05526333, 0.05515146, 0.05514026, 0.05512378,\n",
       "       0.0551208 , 0.05511221, 0.05501353, 0.05500968, 0.0549847 ,\n",
       "       0.0549709 , 0.05496861, 0.05494679, 0.05487379, 0.05481775,\n",
       "       0.05479058, 0.05460422, 0.05456612, 0.05438355, 0.05427985,\n",
       "       0.0542602 , 0.05414438, 0.05396806, 0.0538949 , 0.05344047,\n",
       "       0.05329363, 0.05327445, 0.05327269, 0.05324512, 0.05314672,\n",
       "       0.05307278, 0.0529814 , 0.05295432, 0.05279669, 0.05271975,\n",
       "       0.05261503, 0.05256554, 0.05239647, 0.05221638, 0.05215552,\n",
       "       0.05204444, 0.05185011, 0.05181617, 0.05175835, 0.05175507,\n",
       "       0.05171016, 0.05165974, 0.05161672, 0.05146614, 0.05146449,\n",
       "       0.05144338, 0.05123938, 0.05123462, 0.05123105, 0.05121798,\n",
       "       0.05105206, 0.05098351, 0.05088179, 0.05073218, 0.05070573,\n",
       "       0.05069255, 0.05065148, 0.05059477, 0.05058146, 0.05054818,\n",
       "       0.05049826, 0.05049675, 0.05030641, 0.05024936, 0.05019604,\n",
       "       0.05004277, 0.05003623, 0.05001414, 0.04984267, 0.04967172,\n",
       "       0.04964653, 0.04958618, 0.04952802, 0.04951746, 0.04943068,\n",
       "       0.04927364, 0.04907998, 0.0490788 , 0.04876338, 0.04874351,\n",
       "       0.04872711, 0.04852556, 0.04849245, 0.04842557, 0.04841946,\n",
       "       0.04820363, 0.04789652, 0.04786145, 0.04785348, 0.04776433,\n",
       "       0.047723  , 0.04762202, 0.04726925, 0.04704781, 0.04699937,\n",
       "       0.04692208, 0.04688252, 0.04673687, 0.04673035, 0.04668252,\n",
       "       0.04640101, 0.04632573, 0.04627066, 0.0461515 , 0.04579562,\n",
       "       0.04553593, 0.04548968, 0.04545632, 0.04539191, 0.04535106,\n",
       "       0.04517056, 0.04502737, 0.04500294, 0.04496462, 0.04495121,\n",
       "       0.044863  , 0.04482797, 0.04481931, 0.0446752 , 0.04465057,\n",
       "       0.04460389, 0.04460059, 0.04457141, 0.04454938, 0.04453877,\n",
       "       0.0443855 , 0.04434566, 0.0441755 , 0.0441749 , 0.04403119,\n",
       "       0.04401981, 0.04390153, 0.04386245, 0.04357236, 0.04348962,\n",
       "       0.04341037, 0.04335541, 0.04328381, 0.04316894, 0.04304557,\n",
       "       0.04273591, 0.04273373, 0.04270129, 0.04264156, 0.04260909,\n",
       "       0.0425135 , 0.0424351 , 0.04243381, 0.04237299, 0.04233916,\n",
       "       0.04221317, 0.04218304, 0.04199077, 0.0419712 , 0.04191751,\n",
       "       0.04191623, 0.04178182, 0.04160218, 0.04142548, 0.04131938],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(A_cont[0])[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "2bee0b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9046699930702641)"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.sum(A_opt * rel_mat, axis=1) / k_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "12d805dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.023157894478032476)"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_correlation(mean_allocations, A_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "0f6607e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.,  9.,  4.,  8., 13., 13., 10.,  6.,  6., 12., 11., 11., 21.,\n",
       "        4.,  7.,  8.,  6.,  8., 16.,  5., 15., 13., 20.,  9., 12., 10.,\n",
       "        6.,  2., 11.,  9., 14., 17., 21., 21., 10.,  8., 11.,  5.,  9.,\n",
       "       12.,  9.,  9., 14., 12., 13.,  7.,  9.,  3.,  8.,  6.,  9., 14.,\n",
       "        8., 12.,  9., 15.,  7., 19., 10.,  5., 14.,  9., 10.,  9., 12.,\n",
       "       15.,  9., 11., 10.,  9., 13., 10.,  6., 10.,  7., 10.,  6., 10.,\n",
       "       14.,  6.,  9., 12., 13.,  4.,  9.,  7.,  6.,  9.,  9.,  8., 14.,\n",
       "        9.,  9., 10.,  5., 16., 11.,  8., 10.,  7.,  9., 18., 12.,  4.,\n",
       "       11., 10., 12.,  9.,  4., 18., 14., 18., 17., 16.,  8.,  8., 15.,\n",
       "        7., 11.,  8., 10., 12.,  7.,  6.,  8., 12.,  3., 10.,  4.,  9.,\n",
       "        9., 11., 10., 14.,  4.,  9., 10., 12., 10., 10.,  6., 13.,  9.,\n",
       "       11.,  3.,  7.,  3.,  9.,  8., 14.,  6., 14.,  9.,  9., 10., 17.,\n",
       "       18., 11.,  5.,  9.,  9., 10., 11., 12., 16., 14., 14.,  1.,  5.,\n",
       "       13., 12.,  5., 11., 10.,  8., 12., 16., 13.,  6., 13.,  7., 11.,\n",
       "       11., 12.,  4.,  2.,  4., 14.,  7., 13.,  6.,  7., 11.,  8., 16.,\n",
       "        6.,  8.,  9., 13., 15.], dtype=float32)"
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_opt.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "9197ace8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 1] Epoch  500 | Loss: 625.000 | Util: 0.000 | Violations: 200\n",
      "[Stage 1] Epoch 1000 | Loss: 625.000 | Util: 0.000 | Violations: 200\n",
      "[Stage 2] Epoch 1500 | Loss: 4710.328 | Util: 0.000 | Violations: 200 | Avg Shortfall: 5.000\n",
      "[Stage 2] Epoch 2000 | Loss: 4710.328 | Util: 0.000 | Violations: 200 | Avg Shortfall: 5.000\n",
      "[Stage 2] Epoch 2500 | Loss: 4710.328 | Util: 0.000 | Violations: 200 | Avg Shortfall: 5.000\n",
      "[Stage 2] Epoch 3000 | Loss: 4710.328 | Util: 0.000 | Violations: 200 | Avg Shortfall: 5.000\n",
      "[Stage 2] Epoch 3500 | Loss: 4710.328 | Util: 0.000 | Violations: 200 | Avg Shortfall: 5.000\n",
      "[Stage 2] Epoch 4000 | Loss: 4710.328 | Util: 0.000 | Violations: 200 | Avg Shortfall: 5.000\n",
      "Final violations after projection: 5\n"
     ]
    }
   ],
   "source": [
    "n, m = rel_mat.shape  # ensure rel_matrix is defined\n",
    "R = torch.tensor(rel_mat, dtype=torch.float32, device=\"cpu\")\n",
    "\n",
    "z = (torch.randn(n, m, device=\"cpu\") * 0.01 + 0.1).requires_grad_(True)\n",
    "log_lambda_prod = torch.zeros(m, device=\"cpu\", requires_grad=False)\n",
    "\n",
    "opt = torch.optim.Adam([z], lr=0.05)\n",
    "\n",
    "\n",
    "# STAGE 1: Allow the model to find a meaningful initial solution (no lambdas initially)\n",
    "for epoch in range(1, 1001):\n",
    "    tau = max(0.05, 0.995 ** epoch)\n",
    "    a = torch.sigmoid(z / tau)\n",
    "\n",
    "    util = torch.mean((a * R).sum(dim=1) / k_rec)\n",
    "    L_util = -util\n",
    "\n",
    "    row_sums = a.sum(dim=1)\n",
    "    L_card = ((row_sums - k_rec)**2).mean()\n",
    "\n",
    "    col_sums = a.sum(dim=0)\n",
    "    prod_shortfall = torch.relu(prod_min - col_sums)\n",
    "    L_prod = (prod_shortfall**2).mean()  # soft quadratic penalty, no lambda yet\n",
    "\n",
    "    loss = L_util + 5.0 * L_card + 5.0 * L_prod  # balanced weights, moderate penalties\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        violated_count = (col_sums.detach().cpu().numpy() < prod_min).sum()\n",
    "        print(f\"[Stage 1] Epoch {epoch:4d} | Loss: {loss.item():.3f} | \"\n",
    "              f\"Util: {-L_util.item():.3f} | Violations: {violated_count}\")\n",
    "\n",
    "# STAGE 2: Now introduce controlled dual updates to enforce constraints strictly\n",
    "for epoch in range(1001, 4001):\n",
    "    tau = max(0.01, 0.995 ** epoch)\n",
    "    a = torch.sigmoid(z / tau)\n",
    "\n",
    "    util = torch.mean((a * R).sum(dim=1) / k_rec)\n",
    "    L_util = -util\n",
    "\n",
    "    row_sums = a.sum(dim=1)\n",
    "    L_card = ((row_sums - k_rec)**2).mean()\n",
    "\n",
    "    col_sums = a.sum(dim=0)\n",
    "    prod_shortfall = torch.relu(prod_min - col_sums)\n",
    "\n",
    "    lambda_prod = torch.exp(log_lambda_prod)\n",
    "    L_prod = (lambda_prod * (prod_shortfall**2)).mean()\n",
    "\n",
    "    loss = L_util + 10.0 * L_card + L_prod  # slightly stronger constraints now\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        log_lambda_prod += 0.01 * prod_shortfall  # controlled dual updates\n",
    "        log_lambda_prod.clamp_(min=-2, max=5)\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        violated_count = (col_sums.detach().cpu().numpy() < prod_min).sum()\n",
    "        avg_shortfall = prod_shortfall.mean().item()\n",
    "        print(f\"[Stage 2] Epoch {epoch:4d} | Loss: {loss.item():.3f} | \"\n",
    "              f\"Util: {-L_util.item():.3f} | Violations: {violated_count} | Avg Shortfall: {avg_shortfall:.3f}\")\n",
    "\n",
    "# Final binary projection\n",
    "A_cont = torch.sigmoid(z).detach().cpu().numpy()\n",
    "A_opt = np.zeros_like(A_cont)\n",
    "for i, row in enumerate(A_cont):\n",
    "    A_opt[i, row.argsort()[-k_rec:]] = 1\n",
    "\n",
    "producer_counts = A_opt.sum(axis=0)\n",
    "violations = (producer_counts < prod_min).sum()\n",
    "print(f\"Final violations after projection: {violations}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "044ba255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.8925312649793051)"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.sum(A_opt * rel_mat, axis=1) / k_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "c8ca5073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.,  5., 12.,  9., 13.,  8., 12.,  7., 11.,  6.,  9., 16., 13.,\n",
       "        9.,  8.,  4., 12.,  8.,  9., 12.,  9., 10., 11., 13., 12.,  7.,\n",
       "       11., 10., 10.,  9., 12., 11.,  9., 11., 10., 11., 11., 13.,  7.,\n",
       "        5.,  7.,  8., 14.,  6.,  3., 16., 11., 12.,  8., 13., 10., 13.,\n",
       "        7.,  8.,  6.,  6., 15., 11., 13., 11., 13., 13.,  4.,  9., 16.,\n",
       "       14., 13., 11.,  8., 12., 10., 11., 11., 10.,  9.,  7., 18., 10.,\n",
       "        9., 10., 15., 13.,  8., 11.,  9., 11.,  7., 12., 16., 11., 11.,\n",
       "       10., 13.,  7., 10., 14., 12.,  8., 10., 10.,  6., 14., 15.,  6.,\n",
       "       15., 17.,  6., 18., 12.,  9., 13., 13., 11., 10., 10., 15.,  8.,\n",
       "       10., 11.,  8., 14.,  9., 11.,  7.,  9., 13., 11., 11.,  5.,  6.,\n",
       "        7.,  6.,  8.,  8.,  9.,  9., 10.,  4., 11., 13., 11.,  6.,  9.,\n",
       "        6.,  6.,  9.,  7.,  9.,  7., 10.,  6., 17., 11.,  3.,  7., 19.,\n",
       "        7., 10.,  9., 11., 10.,  6.,  9., 11.,  6.,  9., 11., 13.,  9.,\n",
       "        8.,  8.,  9.,  5., 11., 11., 15.,  7.,  9., 12., 12., 10.,  7.,\n",
       "       14.,  8.,  6.,  8.,  7.,  7.,  7., 12., 14.,  7., 11., 15., 11.,\n",
       "       13., 10., 11., 11.,  6.], dtype=float32)"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_opt.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "fef373e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9466594812127047)"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.sum(mean_allocations * rel_mat, axis=1) / k_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "83a7be40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.88454426 0.82073556 0.92958885 0.95041268 0.90130418 0.95360957\n",
      " 0.90437249 0.90215369 0.84169271 0.81099343 0.74843775 0.96578593\n",
      " 0.94725356 0.95360391 0.89828264 0.8910981  0.78405916 0.89805986\n",
      " 0.94508075 0.97740006 0.79811105 0.87104322 0.98142324 0.85312095\n",
      " 0.8231652  0.91341474 0.89999115 0.8845513  0.9452226  0.85785696\n",
      " 0.91790804 0.97983441 0.91876659 0.89139027 0.89914497 0.8987039\n",
      " 0.81594811 0.89713778 0.82567525 0.91429423 0.9268033  0.91292709\n",
      " 0.90771376 0.97021217 0.92682126 0.94819362 0.8867817  0.87495038\n",
      " 0.93166013 0.7806246  0.73209766 0.78123458 0.92761685 0.86109238\n",
      " 0.96749913 0.79566761 0.89570176 0.93509685 0.9707759  0.72826213\n",
      " 0.84841753 0.72617826 0.92275587 0.95187544 0.91133795 0.87837534\n",
      " 0.89899835 0.80371097 0.71420634 0.93934083 0.80872552 0.78192086\n",
      " 0.92679767 0.93672771 0.96834136 0.74314832 0.73710272 0.84708724\n",
      " 0.88626303 0.84994237 0.88079671 0.79758221 0.81891578 0.87057058\n",
      " 0.97260963 0.97045069 0.7622217  0.96137044 0.73829369 0.96344746\n",
      " 0.88973921 0.82469258 0.58360167 0.87900636 0.76796995 0.75605182\n",
      " 0.98307    0.83221798 0.91599639 0.85516276 0.75216333 0.94905136\n",
      " 0.89920573 0.9679959  0.87263646 0.95255206 0.90563799 0.89101095\n",
      " 0.79797739 0.94706276 0.82360574 0.90061584 0.96498923 0.92985347\n",
      " 0.94607372 0.84307783 0.955918   0.67337864 0.93319916 0.85607468\n",
      " 0.9573425  0.9240877  0.92889848 0.98288104 0.95401157 0.93610961\n",
      " 0.88040985 0.85905188 0.72206079 0.95303008 0.96348601 0.87570441\n",
      " 0.97235428 0.64599618 0.52923471 0.94371783 0.98973409 0.74003518\n",
      " 0.93398385 0.87266274 0.90278946 0.67589982 0.85693599 0.9796301\n",
      " 0.85868077 0.93148637 0.92871379 0.895363   0.8500149  0.90239015\n",
      " 0.95411202 0.61314558 0.8673071  0.88870579 0.82210811 0.95265392\n",
      " 0.90248443 0.94544659 0.85347737 0.90759716 0.84584031 0.736178\n",
      " 0.8994435  0.89422202 0.91600939 0.77311299 0.94977742 0.91987507\n",
      " 0.90829731 0.92432204 0.85570909 0.89708291 0.78007659 0.9050306\n",
      " 0.90513737 0.83419953 0.81794808 0.92697442 0.8188353  0.92701695\n",
      " 0.98203026 0.80019731 0.83259549 0.77375549 0.91181075 0.88339291\n",
      " 0.96273598 0.95451481 0.87918476 0.87964963 0.8700256  0.94111865\n",
      " 0.90035722 0.92834275 0.96181777 0.83687859 0.89369618 0.89750249\n",
      " 0.9639097  0.80669042]\n",
      "[  5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.\n",
      "   5.   5.   5.  27.  14.   5.   5.   5.   5.   5.   5.   5.   5.   5.\n",
      " 116.   5. 115.   5.   5.  56. 139.   5.   5.   5.   5.   5.   5.   5.\n",
      "   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   6.\n",
      "   5.   5.   5.   5.   5.   5.   5.   5.   5. 127.   5.  26.   5.   5.\n",
      "   5.   5.  14.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.\n",
      "   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5. 154.   5.   5.\n",
      "   5.   5.   5.  30.   5.   5.   5.   5.   5.   5.   5.  10.  13.   5.\n",
      "   5.  45.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.\n",
      "   5.   5.   5.   5.   5.  13.   5.   5.   5.   5.   5.   5.   5.   5.\n",
      "   5.   5.   5.   5.   5.   5.   5.   8.   5.  28.   5.   5.   5.   5.\n",
      "   5.  48.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.\n",
      "   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.\n",
      "   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.\n",
      " 111.   5.   5.   5.]\n",
      "[10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      " 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      " 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      " 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      " 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      " 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      " 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      " 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      " 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      " 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      " 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      " 10. 10.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# for each rel_mat row, take top-k based on the allocation\n",
    "sorted_indices = np.argsort(mean_allocations, axis=1)[::-1][:, :k_rec]\n",
    "print(np.mean(rel_mat[np.arange(rel_mat.shape[0])[:, None], sorted_indices], axis=1))\n",
    "print(np.sum(mean_allocations, axis=0))\n",
    "print(np.sum(mean_allocations, axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "b0aef12a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.3194736703774324)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute correlation between the two allocations\n",
    "\n",
    "\n",
    "corr = compute_correlation(new_alls, mean_allocations)\n",
    "corr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
