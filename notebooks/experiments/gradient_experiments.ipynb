{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80fc4297",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import json\n",
    "import cvxpy as cp\n",
    "\n",
    "import torch\n",
    "import torch\n",
    "from src.problems.utils import sample_data_for_group\n",
    "from src.problems.problems import compute_producer_optimal_solution, _compute_consumer_optimal_solution_cvar\n",
    "from src.problems.gradient_problem import compute_consumer_optimal_solution_cvar_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f67f0247",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH_ROOT = Path(\"../../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01ac3b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "with open(DATA_PATH_ROOT / \"amazon_predictions.npy\", \"rb\") as f:\n",
    "    REL_MATRIX = np.load(f)\n",
    "\n",
    "with open(DATA_PATH_ROOT / \"amazon_user_groups.json\", \"r\") as f:\n",
    "    GROUPS_MAP = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9034f784",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CONSUMERS = 400\n",
    "N_PRODUCERS = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56e4e579",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_matrix, consumer_ids, group_assignments = sample_data_for_group(\n",
    "    n_consumers=N_CONSUMERS,\n",
    "    n_producers=N_PRODUCERS,\n",
    "    groups_map=GROUPS_MAP,\n",
    "    group_key=\"top_category\",\n",
    "    data=REL_MATRIX,\n",
    "    seed=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a40aa00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10.0,\n",
       " array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 1., ..., 0., 0., 0.],\n",
       "        [0., 0., 1., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 1.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], shape=(400, 400)))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_producer_optimal_solution(\n",
    "    rel_matrix=sampled_matrix,\n",
    "    k_rec=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4be50f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cvar_allocations = _compute_consumer_optimal_solution_cvar(\n",
    "    rel_matrix=sampled_matrix,\n",
    "    k_rec=10,\n",
    "    producer_max_min_utility=10,\n",
    "    group_assignments=group_assignments,\n",
    "    gamma=0.5,\n",
    "    alpha=0.5,\n",
    "    solver=cp.SCIP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d6e85f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvar_allocations = cvar_allocations.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "828ae27f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9445202392750154)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.sum(cvar_allocations * sampled_matrix, axis=1)) / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0159f63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     1 — loss: 36189.8164, util: 0.00, card: 36189.7539, prod: 0.0000, bin: 0.0625, rho: -0.0010, tau: 0.9990, grad: 5.0000\n",
      "Epoch   500 — loss: 0.0738, util: 0.06, card: 0.0107, prod: 0.0001, bin: 0.0013, rho: -0.0060, tau: 0.6064, grad: 0.6299\n",
      "Epoch  1000 — loss: 0.0575, util: 0.04, card: 0.0125, prod: 0.0000, bin: 0.0007, rho: -0.0060, tau: 0.3677, grad: 0.2614\n",
      "Epoch  1500 — loss: 0.0478, util: 0.02, card: 0.0243, prod: 0.0000, bin: 0.0007, rho: -0.0060, tau: 0.2230, grad: 1.6917\n",
      "Epoch  2000 — loss: 0.0300, util: 0.02, card: 0.0086, prod: 0.0000, bin: 0.0006, rho: -0.0060, tau: 0.1352, grad: 0.3926\n",
      "Epoch  2500 — loss: 0.0243, util: 0.02, card: 0.0062, prod: 0.0001, bin: 0.0006, rho: -0.0060, tau: 0.0820, grad: 0.9120\n",
      "Epoch  3000 — loss: 0.0233, util: 0.02, card: 0.0019, prod: 0.0005, bin: 0.0007, rho: -0.0060, tau: 0.0497, grad: 2.3523\n",
      "Epoch  3500 — loss: 0.0199, util: 0.02, card: 0.0030, prod: 0.0000, bin: 0.0009, rho: -0.0060, tau: 0.0301, grad: 1.5388\n",
      "Epoch  4000 — loss: 0.0195, util: 0.02, card: 0.0031, prod: 0.0000, bin: 0.0011, rho: -0.0060, tau: 0.0183, grad: 1.4286\n",
      "Epoch  4500 — loss: 0.0191, util: 0.01, card: 0.0031, prod: 0.0000, bin: 0.0009, rho: -0.0060, tau: 0.0111, grad: 1.0828\n",
      "Epoch  5000 — loss: 0.0184, util: 0.01, card: 0.0033, prod: 0.0000, bin: 0.0007, rho: -0.0060, tau: 0.0067, grad: 1.1486\n",
      "Epoch  5500 — loss: 0.0177, util: 0.01, card: 0.0030, prod: 0.0000, bin: 0.0004, rho: -0.0060, tau: 0.0041, grad: 0.9360\n",
      "Epoch  6000 — loss: 0.0172, util: 0.01, card: 0.0033, prod: 0.0000, bin: 0.0002, rho: -0.0060, tau: 0.0025, grad: 1.3093\n",
      "Epoch  6500 — loss: 0.0167, util: 0.01, card: 0.0030, prod: 0.0000, bin: 0.0001, rho: -0.0060, tau: 0.0015, grad: 1.3347\n",
      "Epoch  7000 — loss: 0.0164, util: 0.01, card: 0.0029, prod: 0.0000, bin: 0.0001, rho: -0.0060, tau: 0.0010, grad: 1.3573\n",
      "Epoch  7500 — loss: 0.0163, util: 0.01, card: 0.0029, prod: 0.0000, bin: 0.0001, rho: -0.0060, tau: 0.0010, grad: 1.2450\n",
      "Epoch  8000 — loss: 0.0163, util: 0.01, card: 0.0029, prod: 0.0000, bin: 0.0001, rho: -0.0060, tau: 0.0010, grad: 1.0433\n",
      "Epoch  8500 — loss: 0.0163, util: 0.01, card: 0.0029, prod: 0.0000, bin: 0.0001, rho: -0.0060, tau: 0.0010, grad: 1.3263\n",
      "Epoch  9000 — loss: 0.0163, util: 0.01, card: 0.0029, prod: 0.0000, bin: 0.0001, rho: -0.0060, tau: 0.0010, grad: 0.8098\n",
      "Epoch  9500 — loss: 0.0163, util: 0.01, card: 0.0029, prod: 0.0000, bin: 0.0001, rho: -0.0060, tau: 0.0010, grad: 1.1577\n",
      "Epoch 10000 — loss: 0.0163, util: 0.01, card: 0.0029, prod: 0.0000, bin: 0.0001, rho: -0.0060, tau: 0.0010, grad: 0.9392\n",
      "Epoch 10500 — loss: 0.0163, util: 0.01, card: 0.0029, prod: 0.0000, bin: 0.0001, rho: -0.0060, tau: 0.0010, grad: 1.1775\n",
      "Epoch 11000 — loss: 0.0163, util: 0.01, card: 0.0029, prod: 0.0000, bin: 0.0001, rho: -0.0060, tau: 0.0010, grad: 1.5457\n",
      "Epoch 11500 — loss: 0.0163, util: 0.01, card: 0.0029, prod: 0.0000, bin: 0.0001, rho: -0.0060, tau: 0.0010, grad: 1.4147\n"
     ]
    }
   ],
   "source": [
    "allocations = compute_consumer_optimal_solution_cvar_grad(\n",
    "    rel_matrix=sampled_matrix,\n",
    "    k_rec=10,\n",
    "    producer_max_min_utility=10,\n",
    "    gamma=0.5,\n",
    "    group_assignments=group_assignments,\n",
    "    alpha=0.5,\n",
    "    hidden_dim=200,\n",
    "    max_epochs=30000,\n",
    "    verbose=True,\n",
    "    max_patience=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25724b49",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cvar_allocations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcvar_allocations\u001b[49m\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cvar_allocations' is not defined"
     ]
    }
   ],
   "source": [
    "cvar_allocations.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06d22039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  6.301512 ,   4.9992223,   4.998426 ,   4.997872 ,   4.999021 ,\n",
       "         4.998382 ,   4.99619  ,   4.9976606, 220.       ,   4.997788 ,\n",
       "         5.054884 ,   4.9982195,   4.997365 ,   5.166075 ,   6.339166 ,\n",
       "         4.997844 ,   4.9983544,   4.996832 ,   4.9979815,   4.9977555,\n",
       "       365.67273  ,   4.9980345,   4.9999313,   5.0556755,   4.9972286,\n",
       "         5.002362 ,   5.211969 ,   5.288984 ,   4.9979973,   5.024349 ,\n",
       "         4.997323 ,   4.999029 ,   4.998927 ,   4.99877  ,   5.1152034,\n",
       "         4.997915 ,   5.008338 ,   4.9977317,   4.998155 ,   4.998006 ,\n",
       "         4.998737 ,   5.1482334,   5.1303244,   4.9976172,   4.9980426,\n",
       "         4.9981513,   4.998585 ,   4.998369 ,   4.9994283,   4.997899 ,\n",
       "         4.99696  ,   4.9982185,   4.998072 ,   5.0151587,   5.045986 ,\n",
       "         4.996604 ,   4.998085 ,   4.99835  ,   5.1243043,   5.06973  ,\n",
       "         4.998045 ,   5.1189246,   4.9983654,   4.9999375,   4.9983096,\n",
       "         4.9981046,   5.1095753,   4.9976754,   4.998905 ,   4.9986267,\n",
       "         4.9987907,   4.99841  ,   4.9971747,   4.9998646, 218.99901  ,\n",
       "         4.9985185,   4.998954 ,   4.998912 ,   4.998783 ,   4.9985037,\n",
       "         4.9978375,   4.9979916,   4.9988594,   4.997552 ,   4.9977603,\n",
       "         4.998491 ,   4.998348 ,   4.995606 ,   4.9980884,   4.997263 ,\n",
       "         4.997427 ,   5.214824 ,   4.998578 ,   4.9980993,   4.9984508,\n",
       "         4.9985604,   4.998762 ,   4.9988294,   4.997484 ,   4.997094 ,\n",
       "         5.27652  ,   4.9980264,   4.997268 ,   4.998708 ,   4.9988184,\n",
       "         4.997093 ,   4.9967055,   4.9973426,   4.9969683,   4.9968414,\n",
       "         4.9988976,   4.998989 ,   5.137871 ,   4.998726 ,   4.998438 ,\n",
       "         4.99839  ,   5.0842376,   5.0623236,   4.9983115,   4.9979534,\n",
       "         4.9993935,   4.9981356,   4.9969573,   4.9971795,   5.0001664,\n",
       "         4.9983582,   4.9983096,   5.103362 ,   5.118397 ,   4.99665  ,\n",
       "         4.9969306,   4.998405 ,   4.9984083,   4.9983463, 219.99426  ,\n",
       "         5.0614686,   4.999414 , 217.96805  ,   5.2419696,   4.9972587,\n",
       "         4.998521 ,   5.069786 ,   4.997809 ,   5.152596 ,   4.997534 ,\n",
       "         5.0258274,   4.9961624,   4.997489 ,   4.998955 ,   4.997442 ,\n",
       "         4.997573 ,   4.997438 ,   4.998548 ,   4.9969983,   4.997266 ,\n",
       "         4.9981084,   4.9966245,   4.9977922,   4.998163 ,   4.997124 ,\n",
       "         4.9983015,   4.9990945, 303.7164   ,   4.998937 ,   4.997684 ,\n",
       "         4.9981008,   4.998637 ,   4.996018 ,   4.999502 ,   4.997256 ,\n",
       "         4.997589 ,   4.9987764,   4.99733  , 221.97723  ,   4.998709 ,\n",
       "         5.141784 ,   4.998223 ,   4.997885 ,   5.1039414,   6.1710615,\n",
       "         4.997471 ,   5.3230777,   5.1008215,   4.997897 ,   4.997963 ,\n",
       "         4.9971547,   4.9990997,   4.998468 ,   4.9985037,   4.997386 ,\n",
       "         4.999174 ,   6.1944423,   4.997855 ,   4.998455 ,   4.999961 ,\n",
       "         4.9982724,   4.999334 ,   5.0957255,   4.9978704,   4.9973545,\n",
       "         4.9985476,   4.998028 ,   4.998735 ,   4.996525 ,   4.9976735,\n",
       "         4.997173 ,   4.998473 ,   4.9981136,   4.9984035,   4.9981194,\n",
       "         4.9981017,   4.9970436,   4.998352 ,   4.9991016,   4.9973235,\n",
       "         4.9992146,   4.9984546,   4.9985466,   4.998241 ,   4.996756 ,\n",
       "         5.046075 ,   6.0005608,   4.998159 ,   4.999192 ,   4.997598 ,\n",
       "         5.10502  ,   4.9979563,   5.0051126,   4.9974756,   4.9978566,\n",
       "         4.99975  ,   4.9995146,   4.9990406,   4.999978 ,   4.9981093,\n",
       "         4.996544 ,   4.99816  ,   5.2322035,   4.999383 ,   4.9975076,\n",
       "         4.997987 ,   4.9988623,   4.996421 ,   4.9983163,   4.997508 ,\n",
       "         4.9990606,   4.9993167,   5.0018406,   4.998721 ,   6.0771194,\n",
       "         5.2056847,   4.9976697,   4.9996233,   4.9987435,   5.0153174,\n",
       "         4.9996657,   5.0054035,   4.998011 ,   4.998445 ,   4.997658 ,\n",
       "         4.99809  ,   4.999867 ,   4.99876  ,   4.9968185,   4.9979553,\n",
       "         4.999254 ,   5.0000215,   4.9998035,   4.9978433,   4.9982553,\n",
       "         4.996767 ,   4.998066 ,   4.9981265,   4.998571 ,   4.9995656,\n",
       "         4.9983125,   4.9986978,   4.9976287,   5.0543337,   4.9982285,\n",
       "         6.0138946,   4.997353 ,   4.9984217,   4.997904 ,   4.999048 ,\n",
       "         4.9961658,   4.9983153,   4.999545 ,  62.242092 ,   5.2317343,\n",
       "         4.9977546,   4.9998126,   4.998484 ,   4.997631 ,   4.99746  ,\n",
       "         4.9979515,   4.998705 ,   4.999386 ,   4.999567 ,   4.9981594,\n",
       "         4.9978056,   4.998487 ,   4.997728 ,   4.9965706,   4.999159 ,\n",
       "         4.99784  ,   5.082597 ,   4.998784 ,   4.997687 ,   4.999842 ,\n",
       "         5.0000033,   5.131763 ,   5.137781 ,   4.99735  ,   5.001395 ,\n",
       "         4.997782 ,   4.9971447,   4.9974117,   4.9978285,   4.997859 ,\n",
       "         4.9988484,   4.998578 ,   4.998582 ,   4.9995394,   4.9988756,\n",
       "         4.998104 ,   4.999249 ,   4.9997144,   4.998402 ,   4.999146 ,\n",
       "         4.9980316,   4.997682 ,   4.997206 ,   4.998766 ,   4.9988565,\n",
       "         4.997342 ,   4.998881 ,   4.9995775,   4.998523 ,   4.997634 ,\n",
       "         4.9983497,   5.001329 ,   4.9971714,   4.999658 ,   4.998515 ,\n",
       "         4.9974623,   4.9981594,   4.997205 ,   4.9984336,   5.0652666,\n",
       "         4.9983606,   5.0673227,   4.998495 ,   5.260432 ,   4.9983273,\n",
       "         4.9971895,   4.9988465,   4.9985166,   4.998235 ,   4.998104 ,\n",
       "         4.999507 ,   4.9981146,   4.998174 ,   4.997129 , 220.01692  ,\n",
       "         4.9978027,   4.996722 ,   4.9981422,   5.021521 ,   4.997628 ,\n",
       "         4.9969563,   4.999486 ,   4.998392 ,   4.999191 ,   4.9959145,\n",
       "         4.997137 ,   4.999134 ,   4.997361 ,   4.997741 ,   4.9999957,\n",
       "         4.998226 ,   4.999418 ,   4.998414 ,   4.9976754,   5.022096 ,\n",
       "         4.9996853,   4.9979916,   4.9986563,   4.997366 ,   4.998711 ,\n",
       "         4.9970827,   4.9990644,   4.9970894,   4.996789 ,   6.030472 ,\n",
       "         5.001817 ,   4.998683 ,   5.013314 ,   4.997717 ,   4.997161 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allocations.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71b0e60a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  6.,   5.,   5.,   5.,   6.,   5.,   5.,   4., 220.,   5.,   5.,\n",
       "         5.,   5.,   5.,   6.,   5.,   5.,   5.,   5.,   5., 366.,   5.,\n",
       "         5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,\n",
       "         5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,\n",
       "         5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,\n",
       "         5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,\n",
       "         5.,   5.,   5.,   5.,   5.,   5.,   5.,   5., 219.,   5.,   5.,\n",
       "         5.,   5.,   5.,   5.,   5.,   5.,   4.,   5.,   5.,   5.,   5.,\n",
       "         5.,   5.,   5.,   5.,   5.,   5.,   5.,   4.,   4.,   4.,   4.,\n",
       "         5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,\n",
       "         5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,\n",
       "         5.,   5.,   4.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,\n",
       "         5.,   5., 220.,   5.,   5., 218.,   5.,   5.,   5.,   5.,   5.,\n",
       "         5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,\n",
       "         5.,   5.,   5.,   5.,   4.,   5.,   5.,   5., 304.,   5.,   5.,\n",
       "         5.,   5.,   5.,   5.,   5.,   5.,   5.,   5., 222.,   5.,   5.,\n",
       "         5.,   5.,   5.,   6.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,\n",
       "         5.,   5.,   5.,   5.,   6.,   5.,   5.,   5.,   5.,   5.,   5.,\n",
       "         5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,\n",
       "         5.,   5.,   5.,   5.,   5.,   6.,   5.,   5.,   5.,   5.,   5.,\n",
       "         5.,   6.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,\n",
       "         5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,\n",
       "         5.,   5.,   5.,   5.,   5.,   5.,   5.,   6.,   5.,   4.,   5.,\n",
       "         5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,\n",
       "         5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,\n",
       "         5.,   5.,   5.,   5.,   5.,   6.,   5.,   5.,   5.,   5.,   5.,\n",
       "         5.,   5.,  62.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,\n",
       "         5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,\n",
       "         5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,\n",
       "         5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,\n",
       "         5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   4.,\n",
       "         5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,\n",
       "         5.,   5.,   5.,   5.,   5.,   5.,   5.,   4.,   5.,   5.,   5.,\n",
       "         5., 220.,   5.,   4.,   5.,   5.,   5.,   5.,   5.,   4.,   5.,\n",
       "         5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,\n",
       "         5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   5.,   6.,   5.,\n",
       "         5.,   5.,   5.,   5.], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(allocations).sum(axis=0)  # check that each user has k_rec items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d937902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_std_between_groups(out, group_assignments):\n",
    "    unique_groups, group_indices = np.unique(group_assignments, return_inverse=True)\n",
    "    num_groups = len(unique_groups)\n",
    "    group_masks = [group_indices == i for i in range(num_groups)]\n",
    "    group_sizes = np.array([mask.sum() for mask in group_masks])\n",
    "\n",
    "    means = []\n",
    "    for mask, size in zip(group_masks, group_sizes):\n",
    "        group_alloc = out[mask]\n",
    "        mean = np.mean(group_alloc.sum(axis=0))\n",
    "        means.append(mean)\n",
    "\n",
    "    return np.mean(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d46cedd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.5155661581092854)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_std_between_groups(allocations * sampled_matrix, group_assignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "e8d34096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(3.1376325527596727), np.float64(3.1616680929488155))"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_std_between_groups(A_opt * sampled_matrix, group_assignments), compute_std_between_groups(cvar_allocations * sampled_matrix, group_assignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "59c9e29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_cont = model.eval().forward().detach().cpu().numpy()\n",
    "A_opt = np.zeros_like(A_cont)\n",
    "for i, row in enumerate(A_cont):\n",
    "    A_opt[i, row.argsort()[-k_rec:]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "4ed22395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5,   5,   5,   5,  94,   5,   5,   5,   5,   5,   5,   5,   5,\n",
       "         5,   5,   5,   5,   5,  41,   5,   5,   5,   5,   5,   5,   5,\n",
       "         5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,\n",
       "         5,   5,   5,   5,   5,   5,   5,   5,  55,   5,   5,   5,   5,\n",
       "         5,   5,   5,  26,   5,   5,   5,   5,   5,   5,  15,   5,   5,\n",
       "        63,   5,  16,   5,   5, 131,   5,   5,   5,   5,   5,   5,   5,\n",
       "         5,   5,   5,   5,   5,   5,   9,   5,   5,   5,   5,   5,   5,\n",
       "         5,   5,   5,   5,   5,   5,   5,   5,  66,   5,   5,   5,   5,\n",
       "         5,   5,   5,   6,   5,  12,   5,   5,   5,   5,   5,   5,   5,\n",
       "         5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,\n",
       "         5,   5,   5,   5, 157,   6,   5,   5,   5,   5,   5,   5,   5,\n",
       "         5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,\n",
       "        36,   5,   5,   5,   5,   5,   5,   5,  29,   5,   5,   5,   5,\n",
       "         5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,  11,   5,\n",
       "         5,   5, 111,   5,  80,   5,   5,   5,   5,   5,   5,   5, 136,\n",
       "         5,   5,   5,   5,   5])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvar_allocations.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "2e3d6bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5,   5,   5,   5,  86,   5,   5,   5,   5,   5,   5,   5,   5,\n",
       "         5,   5,   5,   5,   5,  19,   5,   5,   5,   5,   5,   5,   5,\n",
       "         5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,\n",
       "         5,   5,   5,   5,   5,   5,   5,   5,  74,   5,   5,   5,   5,\n",
       "         5,   5,   5,   8,   5,   5,   5,   5,   4,   5,   5,   5,   5,\n",
       "        12,   5,   6,   5,   5, 200,   5,   5,   5,   5,   5,   5,   4,\n",
       "         5,   5,   5,   4,   5,   5,   5,   5,   4,   5,   5,   5,   5,\n",
       "         5,   5,   5,   5,   5,   5,   5,   5,  90,   5,   5,   5,   5,\n",
       "         5,   5,   5,   5,   5,  12,   5,   5,   5,   5,   5,   5,   5,\n",
       "         5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,\n",
       "         4,   5,   5,   5, 200,   5,   5,   5,   6,   5,   5,   4,   5,\n",
       "         5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,\n",
       "        13,   5,   5,   5,   5,   5,   5,   5,   6,   5,   5,   5,   5,\n",
       "         5,   5,   5,   5,   5,   5,   4,   5,   5,   5,   5,   5,   5,\n",
       "         5,   5, 142,   5,  13,   5,   5,   5,   5,   5,   5,   5, 200,\n",
       "         5,   5,   5,   5,   5])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_opt.sum(axis=0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "6d538269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9415035986031873)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.sum(A_opt * sampled_matrix, axis=1) / k_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "f2212c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "       10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "       10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "       10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "       10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "       10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "       10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "       10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "       10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "       10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "       10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "       10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "       10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "       10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "       10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "       10., 10., 10., 10., 10.], dtype=float32)"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_opt.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "936472fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.7347368338939393)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_correlation(A_opt, mean_allocations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "514429df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dominykas.seputis/github/msc-thesis/.venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   500 — loss: -8.8511, util: 0.89, card: 0.0004, prod: 0.0000, bin: 0.0476\n",
      "Epoch  1000 — loss: -8.8768, util: 0.89, card: 0.0000, prod: 0.0000, bin: 0.0475\n",
      "Epoch  1500 — loss: -8.8769, util: 0.89, card: 0.0000, prod: 0.0000, bin: 0.0475\n",
      "Epoch  2000 — loss: -8.8771, util: 0.89, card: 0.0000, prod: 0.0000, bin: 0.0475\n",
      "Epoch  2500 — loss: -8.8773, util: 0.89, card: 0.0000, prod: 0.0000, bin: 0.0475\n",
      "Epoch  3000 — loss: -8.8776, util: 0.89, card: 0.0000, prod: 0.0000, bin: 0.0475\n"
     ]
    }
   ],
   "source": [
    "\n",
    "R = torch.tensor(rel_mat)\n",
    "n, m = R.shape\n",
    "z = torch.rand(n, m, requires_grad=True, device=\"cpu\")\n",
    "opt = torch.optim.Adam([z], lr=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, patience=100, factor=0.5, verbose=True)\n",
    "prod_min = 5\n",
    "\n",
    "\n",
    "lambda_util = 10\n",
    "lambda_card = 100\n",
    "lambda_prod = 10\n",
    "k_rec = 10\n",
    "decay = 1e-2\n",
    "\n",
    "for epoch in range(1, 3001):\n",
    "    a = torch.sigmoid(z)\n",
    "\n",
    "    util   = torch.mean((a * R).sum(dim=1) / k_rec)\n",
    "    L_util = -util\n",
    "\n",
    "    row_sums = a.sum(dim=1)\n",
    "    L_card   = torch.mean((row_sums - k_rec)**2)\n",
    "\n",
    "    col_sums = a.sum(dim=0)\n",
    "    L_prod   = torch.mean(torch.relu(prod_min - col_sums)**2)\n",
    "\n",
    "    L_bin  = torch.mean(a * (1 - a))\n",
    "\n",
    "    loss = lambda_util * L_util + lambda_card * L_card + lambda_prod * L_prod + L_bin\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"Epoch {epoch:5d} — loss: {loss.item():.4f}, util: {-L_util.item():.2f}, card: {L_card.item():.4f}, prod: {L_prod.item():.4f}, bin: {L_bin.item():.4f}\")\n",
    "\n",
    "    opt.step()\n",
    "    scheduler.step(loss.item())\n",
    "\n",
    "A_cont = torch.sigmoid(z).detach().cpu().numpy()\n",
    "# projection to binary\n",
    "A_opt = np.zeros_like(A_cont)\n",
    "for i, row in enumerate(A_cont):\n",
    "    A_opt[i, row.argsort()[-k_rec:]] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "20598bb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05779154, 0.05771403, 0.05765368, 0.05763114, 0.05748032,\n",
       "       0.05746737, 0.05745109, 0.0573871 , 0.05737695, 0.05736849,\n",
       "       0.05735333, 0.0573409 , 0.05724973, 0.05698105, 0.0569311 ,\n",
       "       0.05686256, 0.05677284, 0.05677127, 0.05671323, 0.0565905 ,\n",
       "       0.05654738, 0.05640062, 0.0563875 , 0.05636477, 0.05631623,\n",
       "       0.05626244, 0.0561929 , 0.05607791, 0.0560635 , 0.05598119,\n",
       "       0.055796  , 0.05579099, 0.05566983, 0.05551579, 0.05549545,\n",
       "       0.05544922, 0.05526333, 0.05515146, 0.05514026, 0.05512378,\n",
       "       0.0551208 , 0.05511221, 0.05501353, 0.05500968, 0.0549847 ,\n",
       "       0.0549709 , 0.05496861, 0.05494679, 0.05487379, 0.05481775,\n",
       "       0.05479058, 0.05460422, 0.05456612, 0.05438355, 0.05427985,\n",
       "       0.0542602 , 0.05414438, 0.05396806, 0.0538949 , 0.05344047,\n",
       "       0.05329363, 0.05327445, 0.05327269, 0.05324512, 0.05314672,\n",
       "       0.05307278, 0.0529814 , 0.05295432, 0.05279669, 0.05271975,\n",
       "       0.05261503, 0.05256554, 0.05239647, 0.05221638, 0.05215552,\n",
       "       0.05204444, 0.05185011, 0.05181617, 0.05175835, 0.05175507,\n",
       "       0.05171016, 0.05165974, 0.05161672, 0.05146614, 0.05146449,\n",
       "       0.05144338, 0.05123938, 0.05123462, 0.05123105, 0.05121798,\n",
       "       0.05105206, 0.05098351, 0.05088179, 0.05073218, 0.05070573,\n",
       "       0.05069255, 0.05065148, 0.05059477, 0.05058146, 0.05054818,\n",
       "       0.05049826, 0.05049675, 0.05030641, 0.05024936, 0.05019604,\n",
       "       0.05004277, 0.05003623, 0.05001414, 0.04984267, 0.04967172,\n",
       "       0.04964653, 0.04958618, 0.04952802, 0.04951746, 0.04943068,\n",
       "       0.04927364, 0.04907998, 0.0490788 , 0.04876338, 0.04874351,\n",
       "       0.04872711, 0.04852556, 0.04849245, 0.04842557, 0.04841946,\n",
       "       0.04820363, 0.04789652, 0.04786145, 0.04785348, 0.04776433,\n",
       "       0.047723  , 0.04762202, 0.04726925, 0.04704781, 0.04699937,\n",
       "       0.04692208, 0.04688252, 0.04673687, 0.04673035, 0.04668252,\n",
       "       0.04640101, 0.04632573, 0.04627066, 0.0461515 , 0.04579562,\n",
       "       0.04553593, 0.04548968, 0.04545632, 0.04539191, 0.04535106,\n",
       "       0.04517056, 0.04502737, 0.04500294, 0.04496462, 0.04495121,\n",
       "       0.044863  , 0.04482797, 0.04481931, 0.0446752 , 0.04465057,\n",
       "       0.04460389, 0.04460059, 0.04457141, 0.04454938, 0.04453877,\n",
       "       0.0443855 , 0.04434566, 0.0441755 , 0.0441749 , 0.04403119,\n",
       "       0.04401981, 0.04390153, 0.04386245, 0.04357236, 0.04348962,\n",
       "       0.04341037, 0.04335541, 0.04328381, 0.04316894, 0.04304557,\n",
       "       0.04273591, 0.04273373, 0.04270129, 0.04264156, 0.04260909,\n",
       "       0.0425135 , 0.0424351 , 0.04243381, 0.04237299, 0.04233916,\n",
       "       0.04221317, 0.04218304, 0.04199077, 0.0419712 , 0.04191751,\n",
       "       0.04191623, 0.04178182, 0.04160218, 0.04142548, 0.04131938],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(A_cont[0])[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "2bee0b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9046699930702641)"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.sum(A_opt * rel_mat, axis=1) / k_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "12d805dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.023157894478032476)"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_correlation(mean_allocations, A_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "0f6607e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.,  9.,  4.,  8., 13., 13., 10.,  6.,  6., 12., 11., 11., 21.,\n",
       "        4.,  7.,  8.,  6.,  8., 16.,  5., 15., 13., 20.,  9., 12., 10.,\n",
       "        6.,  2., 11.,  9., 14., 17., 21., 21., 10.,  8., 11.,  5.,  9.,\n",
       "       12.,  9.,  9., 14., 12., 13.,  7.,  9.,  3.,  8.,  6.,  9., 14.,\n",
       "        8., 12.,  9., 15.,  7., 19., 10.,  5., 14.,  9., 10.,  9., 12.,\n",
       "       15.,  9., 11., 10.,  9., 13., 10.,  6., 10.,  7., 10.,  6., 10.,\n",
       "       14.,  6.,  9., 12., 13.,  4.,  9.,  7.,  6.,  9.,  9.,  8., 14.,\n",
       "        9.,  9., 10.,  5., 16., 11.,  8., 10.,  7.,  9., 18., 12.,  4.,\n",
       "       11., 10., 12.,  9.,  4., 18., 14., 18., 17., 16.,  8.,  8., 15.,\n",
       "        7., 11.,  8., 10., 12.,  7.,  6.,  8., 12.,  3., 10.,  4.,  9.,\n",
       "        9., 11., 10., 14.,  4.,  9., 10., 12., 10., 10.,  6., 13.,  9.,\n",
       "       11.,  3.,  7.,  3.,  9.,  8., 14.,  6., 14.,  9.,  9., 10., 17.,\n",
       "       18., 11.,  5.,  9.,  9., 10., 11., 12., 16., 14., 14.,  1.,  5.,\n",
       "       13., 12.,  5., 11., 10.,  8., 12., 16., 13.,  6., 13.,  7., 11.,\n",
       "       11., 12.,  4.,  2.,  4., 14.,  7., 13.,  6.,  7., 11.,  8., 16.,\n",
       "        6.,  8.,  9., 13., 15.], dtype=float32)"
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_opt.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "9197ace8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 1] Epoch  500 | Loss: 625.000 | Util: 0.000 | Violations: 200\n",
      "[Stage 1] Epoch 1000 | Loss: 625.000 | Util: 0.000 | Violations: 200\n",
      "[Stage 2] Epoch 1500 | Loss: 4710.328 | Util: 0.000 | Violations: 200 | Avg Shortfall: 5.000\n",
      "[Stage 2] Epoch 2000 | Loss: 4710.328 | Util: 0.000 | Violations: 200 | Avg Shortfall: 5.000\n",
      "[Stage 2] Epoch 2500 | Loss: 4710.328 | Util: 0.000 | Violations: 200 | Avg Shortfall: 5.000\n",
      "[Stage 2] Epoch 3000 | Loss: 4710.328 | Util: 0.000 | Violations: 200 | Avg Shortfall: 5.000\n",
      "[Stage 2] Epoch 3500 | Loss: 4710.328 | Util: 0.000 | Violations: 200 | Avg Shortfall: 5.000\n",
      "[Stage 2] Epoch 4000 | Loss: 4710.328 | Util: 0.000 | Violations: 200 | Avg Shortfall: 5.000\n",
      "Final violations after projection: 5\n"
     ]
    }
   ],
   "source": [
    "n, m = rel_mat.shape  # ensure rel_matrix is defined\n",
    "R = torch.tensor(rel_mat, dtype=torch.float32, device=\"cpu\")\n",
    "\n",
    "z = (torch.randn(n, m, device=\"cpu\") * 0.01 + 0.1).requires_grad_(True)\n",
    "log_lambda_prod = torch.zeros(m, device=\"cpu\", requires_grad=False)\n",
    "\n",
    "opt = torch.optim.Adam([z], lr=0.05)\n",
    "\n",
    "\n",
    "# STAGE 1: Allow the model to find a meaningful initial solution (no lambdas initially)\n",
    "for epoch in range(1, 1001):\n",
    "    tau = max(0.05, 0.995 ** epoch)\n",
    "    a = torch.sigmoid(z / tau)\n",
    "\n",
    "    util = torch.mean((a * R).sum(dim=1) / k_rec)\n",
    "    L_util = -util\n",
    "\n",
    "    row_sums = a.sum(dim=1)\n",
    "    L_card = ((row_sums - k_rec)**2).mean()\n",
    "\n",
    "    col_sums = a.sum(dim=0)\n",
    "    prod_shortfall = torch.relu(prod_min - col_sums)\n",
    "    L_prod = (prod_shortfall**2).mean()  # soft quadratic penalty, no lambda yet\n",
    "\n",
    "    loss = L_util + 5.0 * L_card + 5.0 * L_prod  # balanced weights, moderate penalties\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        violated_count = (col_sums.detach().cpu().numpy() < prod_min).sum()\n",
    "        print(f\"[Stage 1] Epoch {epoch:4d} | Loss: {loss.item():.3f} | \"\n",
    "              f\"Util: {-L_util.item():.3f} | Violations: {violated_count}\")\n",
    "\n",
    "# STAGE 2: Now introduce controlled dual updates to enforce constraints strictly\n",
    "for epoch in range(1001, 4001):\n",
    "    tau = max(0.01, 0.995 ** epoch)\n",
    "    a = torch.sigmoid(z / tau)\n",
    "\n",
    "    util = torch.mean((a * R).sum(dim=1) / k_rec)\n",
    "    L_util = -util\n",
    "\n",
    "    row_sums = a.sum(dim=1)\n",
    "    L_card = ((row_sums - k_rec)**2).mean()\n",
    "\n",
    "    col_sums = a.sum(dim=0)\n",
    "    prod_shortfall = torch.relu(prod_min - col_sums)\n",
    "\n",
    "    lambda_prod = torch.exp(log_lambda_prod)\n",
    "    L_prod = (lambda_prod * (prod_shortfall**2)).mean()\n",
    "\n",
    "    loss = L_util + 10.0 * L_card + L_prod  # slightly stronger constraints now\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        log_lambda_prod += 0.01 * prod_shortfall  # controlled dual updates\n",
    "        log_lambda_prod.clamp_(min=-2, max=5)\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        violated_count = (col_sums.detach().cpu().numpy() < prod_min).sum()\n",
    "        avg_shortfall = prod_shortfall.mean().item()\n",
    "        print(f\"[Stage 2] Epoch {epoch:4d} | Loss: {loss.item():.3f} | \"\n",
    "              f\"Util: {-L_util.item():.3f} | Violations: {violated_count} | Avg Shortfall: {avg_shortfall:.3f}\")\n",
    "\n",
    "# Final binary projection\n",
    "A_cont = torch.sigmoid(z).detach().cpu().numpy()\n",
    "A_opt = np.zeros_like(A_cont)\n",
    "for i, row in enumerate(A_cont):\n",
    "    A_opt[i, row.argsort()[-k_rec:]] = 1\n",
    "\n",
    "producer_counts = A_opt.sum(axis=0)\n",
    "violations = (producer_counts < prod_min).sum()\n",
    "print(f\"Final violations after projection: {violations}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "044ba255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.8925312649793051)"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.sum(A_opt * rel_mat, axis=1) / k_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "c8ca5073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.,  5., 12.,  9., 13.,  8., 12.,  7., 11.,  6.,  9., 16., 13.,\n",
       "        9.,  8.,  4., 12.,  8.,  9., 12.,  9., 10., 11., 13., 12.,  7.,\n",
       "       11., 10., 10.,  9., 12., 11.,  9., 11., 10., 11., 11., 13.,  7.,\n",
       "        5.,  7.,  8., 14.,  6.,  3., 16., 11., 12.,  8., 13., 10., 13.,\n",
       "        7.,  8.,  6.,  6., 15., 11., 13., 11., 13., 13.,  4.,  9., 16.,\n",
       "       14., 13., 11.,  8., 12., 10., 11., 11., 10.,  9.,  7., 18., 10.,\n",
       "        9., 10., 15., 13.,  8., 11.,  9., 11.,  7., 12., 16., 11., 11.,\n",
       "       10., 13.,  7., 10., 14., 12.,  8., 10., 10.,  6., 14., 15.,  6.,\n",
       "       15., 17.,  6., 18., 12.,  9., 13., 13., 11., 10., 10., 15.,  8.,\n",
       "       10., 11.,  8., 14.,  9., 11.,  7.,  9., 13., 11., 11.,  5.,  6.,\n",
       "        7.,  6.,  8.,  8.,  9.,  9., 10.,  4., 11., 13., 11.,  6.,  9.,\n",
       "        6.,  6.,  9.,  7.,  9.,  7., 10.,  6., 17., 11.,  3.,  7., 19.,\n",
       "        7., 10.,  9., 11., 10.,  6.,  9., 11.,  6.,  9., 11., 13.,  9.,\n",
       "        8.,  8.,  9.,  5., 11., 11., 15.,  7.,  9., 12., 12., 10.,  7.,\n",
       "       14.,  8.,  6.,  8.,  7.,  7.,  7., 12., 14.,  7., 11., 15., 11.,\n",
       "       13., 10., 11., 11.,  6.], dtype=float32)"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_opt.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "fef373e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9466594812127047)"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.sum(mean_allocations * rel_mat, axis=1) / k_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "83a7be40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.88454426 0.82073556 0.92958885 0.95041268 0.90130418 0.95360957\n",
      " 0.90437249 0.90215369 0.84169271 0.81099343 0.74843775 0.96578593\n",
      " 0.94725356 0.95360391 0.89828264 0.8910981  0.78405916 0.89805986\n",
      " 0.94508075 0.97740006 0.79811105 0.87104322 0.98142324 0.85312095\n",
      " 0.8231652  0.91341474 0.89999115 0.8845513  0.9452226  0.85785696\n",
      " 0.91790804 0.97983441 0.91876659 0.89139027 0.89914497 0.8987039\n",
      " 0.81594811 0.89713778 0.82567525 0.91429423 0.9268033  0.91292709\n",
      " 0.90771376 0.97021217 0.92682126 0.94819362 0.8867817  0.87495038\n",
      " 0.93166013 0.7806246  0.73209766 0.78123458 0.92761685 0.86109238\n",
      " 0.96749913 0.79566761 0.89570176 0.93509685 0.9707759  0.72826213\n",
      " 0.84841753 0.72617826 0.92275587 0.95187544 0.91133795 0.87837534\n",
      " 0.89899835 0.80371097 0.71420634 0.93934083 0.80872552 0.78192086\n",
      " 0.92679767 0.93672771 0.96834136 0.74314832 0.73710272 0.84708724\n",
      " 0.88626303 0.84994237 0.88079671 0.79758221 0.81891578 0.87057058\n",
      " 0.97260963 0.97045069 0.7622217  0.96137044 0.73829369 0.96344746\n",
      " 0.88973921 0.82469258 0.58360167 0.87900636 0.76796995 0.75605182\n",
      " 0.98307    0.83221798 0.91599639 0.85516276 0.75216333 0.94905136\n",
      " 0.89920573 0.9679959  0.87263646 0.95255206 0.90563799 0.89101095\n",
      " 0.79797739 0.94706276 0.82360574 0.90061584 0.96498923 0.92985347\n",
      " 0.94607372 0.84307783 0.955918   0.67337864 0.93319916 0.85607468\n",
      " 0.9573425  0.9240877  0.92889848 0.98288104 0.95401157 0.93610961\n",
      " 0.88040985 0.85905188 0.72206079 0.95303008 0.96348601 0.87570441\n",
      " 0.97235428 0.64599618 0.52923471 0.94371783 0.98973409 0.74003518\n",
      " 0.93398385 0.87266274 0.90278946 0.67589982 0.85693599 0.9796301\n",
      " 0.85868077 0.93148637 0.92871379 0.895363   0.8500149  0.90239015\n",
      " 0.95411202 0.61314558 0.8673071  0.88870579 0.82210811 0.95265392\n",
      " 0.90248443 0.94544659 0.85347737 0.90759716 0.84584031 0.736178\n",
      " 0.8994435  0.89422202 0.91600939 0.77311299 0.94977742 0.91987507\n",
      " 0.90829731 0.92432204 0.85570909 0.89708291 0.78007659 0.9050306\n",
      " 0.90513737 0.83419953 0.81794808 0.92697442 0.8188353  0.92701695\n",
      " 0.98203026 0.80019731 0.83259549 0.77375549 0.91181075 0.88339291\n",
      " 0.96273598 0.95451481 0.87918476 0.87964963 0.8700256  0.94111865\n",
      " 0.90035722 0.92834275 0.96181777 0.83687859 0.89369618 0.89750249\n",
      " 0.9639097  0.80669042]\n",
      "[  5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.\n",
      "   5.   5.   5.  27.  14.   5.   5.   5.   5.   5.   5.   5.   5.   5.\n",
      " 116.   5. 115.   5.   5.  56. 139.   5.   5.   5.   5.   5.   5.   5.\n",
      "   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   6.\n",
      "   5.   5.   5.   5.   5.   5.   5.   5.   5. 127.   5.  26.   5.   5.\n",
      "   5.   5.  14.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.\n",
      "   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5. 154.   5.   5.\n",
      "   5.   5.   5.  30.   5.   5.   5.   5.   5.   5.   5.  10.  13.   5.\n",
      "   5.  45.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.\n",
      "   5.   5.   5.   5.   5.  13.   5.   5.   5.   5.   5.   5.   5.   5.\n",
      "   5.   5.   5.   5.   5.   5.   5.   8.   5.  28.   5.   5.   5.   5.\n",
      "   5.  48.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.\n",
      "   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.\n",
      "   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.   5.\n",
      " 111.   5.   5.   5.]\n",
      "[10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      " 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      " 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      " 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      " 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      " 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      " 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      " 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      " 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      " 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      " 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      " 10. 10.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# for each rel_mat row, take top-k based on the allocation\n",
    "sorted_indices = np.argsort(mean_allocations, axis=1)[::-1][:, :k_rec]\n",
    "print(np.mean(rel_mat[np.arange(rel_mat.shape[0])[:, None], sorted_indices], axis=1))\n",
    "print(np.sum(mean_allocations, axis=0))\n",
    "print(np.sum(mean_allocations, axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "b0aef12a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.3194736703774324)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute correlation between the two allocations\n",
    "\n",
    "\n",
    "corr = compute_correlation(new_alls, mean_allocations)\n",
    "corr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
