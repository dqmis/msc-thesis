{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/ml-100k/u.data\", sep=\"\\t\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing negative ratings\n",
    "df = df[df[2] > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[2], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=[0, 3], ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9l/_y964wyn5fx6bcqmbqjpd9780000gp/T/ipykernel_60347/3749236089.py:2: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train = df.groupby(0).apply(lambda x: x.iloc[:-int(len(x) * 0.2)])\n",
      "/var/folders/9l/_y964wyn5fx6bcqmbqjpd9780000gp/T/ipykernel_60347/3749236089.py:3: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  test = df.groupby(0).apply(lambda x: x.iloc[-int(len(x) * 0.2):])\n"
     ]
    }
   ],
   "source": [
    "# split data grouping by user, where 20% of the data ordered by timestamp is used for testing\n",
    "train = df.groupby(0).apply(lambda x: x.iloc[:-int(len(x) * 0.2)])\n",
    "test = df.groupby(0).apply(lambda x: x.iloc[-int(len(x) * 0.2):])\n",
    "\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9l/_y964wyn5fx6bcqmbqjpd9780000gp/T/ipykernel_60347/2676113635.py:2: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train = train.groupby(0).apply(lambda x: x[1].tolist()).tolist()\n",
      "/var/folders/9l/_y964wyn5fx6bcqmbqjpd9780000gp/T/ipykernel_60347/2676113635.py:3: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  test = test.groupby(0).apply(lambda x: x[1].tolist()).tolist()\n"
     ]
    }
   ],
   "source": [
    "# format data into rows of user, item\n",
    "train = train.groupby(0).apply(lambda x: x[1].tolist()).tolist()\n",
    "test = test.groupby(0).apply(lambda x: x[1].tolist()).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if list has more than 10 items, split it into multiple lists of 10 items\n",
    "\n",
    "def split_inner_lists(train, chunk_size):\n",
    "    \"\"\"Splits each inner list in train into sublists of specified chunk size.\"\"\"\n",
    "    result = [sublist[i:i + chunk_size] for sublist in train for i in range(0, len(sublist), chunk_size)]\n",
    "    return result\n",
    "\n",
    "train = split_inner_lists(train, 10)\n",
    "test = split_inner_lists(test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieLensDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = torch.tensor(self.data[idx][:-1], dtype=torch.long)\n",
    "        target = torch.tensor(self.data[idx][1:], dtype=torch.long)\n",
    "\n",
    "        return inputs, target\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        inputs, target = zip(*batch)\n",
    "        max_len = max(map(len, inputs))\n",
    "        inputs = torch.stack([torch.nn.functional.pad(i, (0, max_len - len(i))) for i in inputs])\n",
    "        target = torch.stack([torch.nn.functional.pad(t, (0, max_len - len(t))) for t in target])\n",
    "\n",
    "        return inputs, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size=33278, ninp=200, nhid=200, nlayers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(vocab_size, ninp)\n",
    "        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout, batch_first=True)\n",
    "        self.decoder = nn.Linear(nhid, vocab_size)\n",
    "\n",
    "        self.nlayers = nlayers\n",
    "        self.nhid = nhid\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n",
    "        nn.init.zeros_(self.decoder.bias)\n",
    "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output)\n",
    "        decoded = decoded.view(-1, self.vocab_size)\n",
    "        return F.log_softmax(decoded, dim=1), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "        return (\n",
    "            weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
    "            weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type       | Params | Mode \n",
      "---------------------------------------------\n",
      "0 | model | SimpleLSTM | 1.3 M  | train\n",
      "---------------------------------------------\n",
      "1.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 M     Total params\n",
      "5.272     Total estimated model params size (MB)\n",
      "5         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/Users/dominykas.seputis/github/msc-thesis/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 110/110 [00:00<00:00, 125.96it/s, v_num=55, train_loss=5.080]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 110/110 [00:00<00:00, 123.62it/s, v_num=55, train_loss=5.080]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from lightning.pytorch.demos import SequenceSampler\n",
    "import lightning as L\n",
    "\n",
    "\n",
    "class LanguageModel(L.LightningModule):\n",
    "    def __init__(self, vocab_size=33278):\n",
    "        super().__init__()\n",
    "        self.model = SimpleLSTM(vocab_size)\n",
    "        self.hidden = None\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        self.hidden = None\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input, target = batch\n",
    "        if self.hidden is None:\n",
    "            self.hidden = self.model.init_hidden(input.size(0))\n",
    "        self.hidden = (self.hidden[0].detach(), self.hidden[1].detach())\n",
    "        output, self.hidden = self.model(input, self.hidden)\n",
    "        loss = torch.nn.functional.nll_loss(output, target.view(-1), ignore_index=0)\n",
    "\n",
    "\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def predict(self, input):\n",
    "        hidden = self.model.init_hidden(1)\n",
    "        output, _ = self.model(input, hidden)\n",
    "        return output.squeeze().exp()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.model.parameters(), lr=0.001)\n",
    "\n",
    "dataset = MovieLensDataset(train)\n",
    "dataloader = DataLoader(dataset, batch_sampler=SequenceSampler(dataset, batch_size=64), collate_fn=dataset.collate_fn)\n",
    "model = LanguageModel(vocab_size=1683)\n",
    "trainer = L.Trainer(gradient_clip_val=0.25, max_epochs=20)\n",
    "trainer.fit(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv(\"../data/ml-100k/u.item\", sep=\"|\", header=None, encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary of movie id to movie name\n",
    "movie_dict = dict(zip(movies[0], movies[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows with less than 3 records\n",
    "test = [i for i in test if len(i) > 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at 10: 0.040804918949133594\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# randomly shuffle test data\n",
    "import random\n",
    "random.shuffle(test)\n",
    "\n",
    "test_dataset = MovieLensDataset(test)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_sampler=SequenceSampler(test_dataset, batch_size=1))\n",
    "\n",
    "\n",
    "acc_at_10 = []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    input, target = batch\n",
    "    # get next target\n",
    "    target = target[0][-1]\n",
    "    output = model.predict(input)[0]\n",
    "    target = input[0][-1].item()\n",
    "    topk = torch.topk(output, 10).indices.tolist()\n",
    "    acc_at_10.append(target in topk)\n",
    "\n",
    "\n",
    "print(f\"Accuracy at 10: {sum(acc_at_10) / len(acc_at_10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last three movies watched:\n",
      "Sleepers (1996)\n",
      "Ghost (1990)\n",
      "Aliens (1986)\n",
      "\n",
      "\n",
      "Next movie to watch:\n",
      "Boys of St. Vincent, The (1993)\n",
      "\n",
      "\n",
      "Top 10 recommendations:\n",
      "Carrie (1976)\n",
      "Omen, The (1976)\n",
      "Cape Fear (1991)\n",
      "American Werewolf in London, An (1981)\n",
      "Copycat (1995)\n",
      "Cinderella (1950)\n",
      "Sound of Music, The (1965)\n",
      "Mary Poppins (1964)\n",
      "Groundhog Day (1993)\n",
      "Dolores Claiborne (1994)\n",
      "--------\n",
      "Last three movies watched:\n",
      "Bananas (1971)\n",
      "Clerks (1994)\n",
      "Dances with Wolves (1990)\n",
      "\n",
      "\n",
      "Next movie to watch:\n",
      "Back to the Future (1985)\n",
      "\n",
      "\n",
      "Top 10 recommendations:\n",
      "Empire Strikes Back, The (1980)\n",
      "Pulp Fiction (1994)\n",
      "Raiders of the Lost Ark (1981)\n",
      "Princess Bride, The (1987)\n",
      "Back to the Future (1985)\n",
      "Blues Brothers, The (1980)\n",
      "Aliens (1986)\n",
      "Terminator, The (1984)\n",
      "Jaws (1975)\n",
      "Sting, The (1973)\n",
      "--------\n",
      "Last three movies watched:\n",
      "Caught (1996)\n",
      "Mr. Holland's Opus (1995)\n",
      "Night on Earth (1991)\n",
      "\n",
      "\n",
      "Next movie to watch:\n",
      "Go Fish (1994)\n",
      "\n",
      "\n",
      "Top 10 recommendations:\n",
      "Great White Hype, The (1996)\n",
      "Maverick (1994)\n",
      "Grease (1978)\n",
      "Nine Months (1995)\n",
      "Mask, The (1994)\n",
      "I.Q. (1994)\n",
      "James and the Giant Peach (1996)\n",
      "Mrs. Doubtfire (1993)\n",
      "Forget Paris (1995)\n",
      "Corrina, Corrina (1994)\n",
      "--------\n",
      "Last three movies watched:\n",
      "Fugitive, The (1993)\n",
      "Scream (1996)\n",
      "Boogie Nights (1997)\n",
      "\n",
      "\n",
      "Next movie to watch:\n",
      "Abyss, The (1989)\n",
      "\n",
      "\n",
      "Top 10 recommendations:\n",
      "Carrie (1976)\n",
      "Cape Fear (1991)\n",
      "Omen, The (1976)\n",
      "American Werewolf in London, An (1981)\n",
      "Bram Stoker's Dracula (1992)\n",
      "Nightmare on Elm Street, A (1984)\n",
      "Copycat (1995)\n",
      "Interview with the Vampire (1994)\n",
      "Dolores Claiborne (1994)\n",
      "Cinderella (1950)\n",
      "--------\n",
      "Last three movies watched:\n",
      "Dumb & Dumber (1994)\n",
      "Sgt. Bilko (1996)\n",
      "My Left Foot (1989)\n",
      "\n",
      "\n",
      "Next movie to watch:\n",
      "Ghost (1990)\n",
      "\n",
      "\n",
      "Top 10 recommendations:\n",
      "Independence Day (ID4) (1996)\n",
      "Mission: Impossible (1996)\n",
      "Grease (1978)\n",
      "Ransom (1996)\n",
      "Batman (1989)\n",
      "Willy Wonka and the Chocolate Factory (1971)\n",
      "Broken Arrow (1996)\n",
      "Groundhog Day (1993)\n",
      "Twister (1996)\n",
      "Maverick (1994)\n",
      "--------\n",
      "Last three movies watched:\n",
      "Bound (1996)\n",
      "Rosencrantz and Guildenstern Are Dead (1990)\n",
      "Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1963)\n",
      "\n",
      "\n",
      "Next movie to watch:\n",
      "Wings of Desire (1987)\n",
      "\n",
      "\n",
      "Top 10 recommendations:\n",
      "Streetcar Named Desire, A (1951)\n",
      "Nikita (La Femme Nikita) (1990)\n",
      "Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1963)\n",
      "Cinema Paradiso (1988)\n",
      "Cyrano de Bergerac (1990)\n",
      "Room with a View, A (1986)\n",
      "Third Man, The (1949)\n",
      "Diva (1981)\n",
      "Ran (1985)\n",
      "Some Like It Hot (1959)\n",
      "--------\n",
      "Last three movies watched:\n",
      "Denise Calls Up (1995)\n",
      "Booty Call (1997)\n",
      "Twelve Monkeys (1995)\n",
      "\n",
      "\n",
      "Next movie to watch:\n",
      "Raising Arizona (1987)\n",
      "\n",
      "\n",
      "Top 10 recommendations:\n",
      "Pulp Fiction (1994)\n",
      "Empire Strikes Back, The (1980)\n",
      "Sting, The (1973)\n",
      "Princess Bride, The (1987)\n",
      "Raiders of the Lost Ark (1981)\n",
      "Shawshank Redemption, The (1994)\n",
      "Back to the Future (1985)\n",
      "Monty Python and the Holy Grail (1974)\n",
      "Jaws (1975)\n",
      "Silence of the Lambs, The (1991)\n",
      "--------\n",
      "Last three movies watched:\n",
      "What's Eating Gilbert Grape (1993)\n",
      "Three Colors: White (1994)\n",
      "North by Northwest (1959)\n",
      "\n",
      "\n",
      "Next movie to watch:\n",
      "Graduate, The (1967)\n",
      "\n",
      "\n",
      "Top 10 recommendations:\n",
      "Rear Window (1954)\n",
      "Pulp Fiction (1994)\n",
      "Raiders of the Lost Ark (1981)\n",
      "Graduate, The (1967)\n",
      "Usual Suspects, The (1995)\n",
      "Princess Bride, The (1987)\n",
      "Empire Strikes Back, The (1980)\n",
      "Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1963)\n",
      "Citizen Kane (1941)\n",
      "Psycho (1960)\n",
      "--------\n",
      "Last three movies watched:\n",
      "Sleepless in Seattle (1993)\n",
      "Bob Roberts (1992)\n",
      "Desperado (1995)\n",
      "\n",
      "\n",
      "Next movie to watch:\n",
      "Alien (1979)\n",
      "\n",
      "\n",
      "Top 10 recommendations:\n",
      "Empire Strikes Back, The (1980)\n",
      "Pulp Fiction (1994)\n",
      "Raiders of the Lost Ark (1981)\n",
      "Godfather: Part II, The (1974)\n",
      "Aliens (1986)\n",
      "Godfather, The (1972)\n",
      "Terminator, The (1984)\n",
      "Blade Runner (1982)\n",
      "Return of the Jedi (1983)\n",
      "Jaws (1975)\n",
      "--------\n",
      "Last three movies watched:\n",
      "Santa Clause, The (1994)\n",
      "I.Q. (1994)\n",
      "Multiplicity (1996)\n",
      "\n",
      "\n",
      "Next movie to watch:\n",
      "One Fine Day (1996)\n",
      "\n",
      "\n",
      "Top 10 recommendations:\n",
      "Independence Day (ID4) (1996)\n",
      "Twister (1996)\n",
      "Mission: Impossible (1996)\n",
      "Broken Arrow (1996)\n",
      "Rock, The (1996)\n",
      "Phenomenon (1996)\n",
      "Eraser (1996)\n",
      "Birdcage, The (1996)\n",
      "Ransom (1996)\n",
      "Long Kiss Goodnight, The (1996)\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "# get some sample predictions\n",
    "\n",
    "import random\n",
    "random.shuffle(test)\n",
    "\n",
    "for i in range(10):\n",
    "    input = torch.tensor(test[i][:-1]).unsqueeze(0)\n",
    "    target = test[i][0]\n",
    "\n",
    "    output = model.predict(input)[0]\n",
    "    topk = torch.topk(output, 10).indices.tolist()\n",
    "\n",
    "    print(\"Last three movies watched:\")\n",
    "    for movie in input[0][-3:]:\n",
    "        print(movie_dict[movie.item()])\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"Next movie to watch:\")\n",
    "    print(movie_dict[target])\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"Top 10 recommendations:\")\n",
    "    for movie in topk:\n",
    "        print(movie_dict[movie])\n",
    "\n",
    "    print(\"--------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type                     | Params | Mode \n",
      "-----------------------------------------------------------\n",
      "0 | model | SimpleDecoderTransformer | 1.5 M  | train\n",
      "-----------------------------------------------------------\n",
      "1.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 M     Total params\n",
      "5.925     Total estimated model params size (MB)\n",
      "35        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "`Trainer.fit` stopped: No training batches.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from lightning.pytorch.demos import SequenceSampler\n",
    "import lightning as L\n",
    "\n",
    "class MovieLensDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        watched_sequence = torch.tensor(self.data[idx][:-1])\n",
    "        next_movie = torch.tensor(self.data[idx][-1])\n",
    "        return watched_sequence, next_movie\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        watched_sequences, next_movies = zip(*batch)\n",
    "        max_len = max(map(len, watched_sequences))\n",
    "        watched_sequences = torch.stack([torch.nn.functional.pad(i, (0, max_len - len(i))) for i in watched_sequences])\n",
    "\n",
    "        return watched_sequences, torch.tensor(next_movies)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(1)].transpose(0, 1)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class SimpleDecoderTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size=33278, ninp=200, nhid=200, nhead=2, nlayers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed = nn.Embedding(vocab_size, ninp)\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=ninp, nhead=nhead, dim_feedforward=nhid, dropout=dropout, batch_first=True)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, nlayers)\n",
    "        self.decoder = nn.Linear(ninp, vocab_size)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.embed.weight, -initrange, initrange)\n",
    "        nn.init.zeros_(self.decoder.bias)\n",
    "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.embed(src) * math.sqrt(self.embed.embedding_dim)\n",
    "        embedded = self.pos_encoder(embedded)\n",
    "\n",
    "        seq_len = embedded.size(1)\n",
    "\n",
    "        # Causal mask\n",
    "        causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool().to(embedded.device)\n",
    "\n",
    "        # Because we lack an encoder, create a dummy memory filled with zeros\n",
    "        dummy_memory = torch.zeros_like(embedded)\n",
    "\n",
    "        output = self.transformer_decoder(embedded, dummy_memory, tgt_mask=causal_mask)\n",
    "        output = self.decoder(output)\n",
    "        return F.log_softmax(output, dim=-1)\n",
    "\n",
    "\n",
    "class LanguageModel(L.LightningModule):\n",
    "    def __init__(self, vocab_size=33278):\n",
    "        super().__init__()\n",
    "        self.model = SimpleDecoderTransformer(vocab_size)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, target = batch\n",
    "        output = self.model(inputs)\n",
    "        # We only care about the prediction after the last observed movie in sequences\n",
    "        last_output = output[:, -1, :]\n",
    "        loss = torch.nn.functional.nll_loss(last_output, target, ignore_index=0)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.model.parameters(), lr=0.001)\n",
    "\n",
    "    def predict_next(self, input_sequence):\n",
    "        # Assume input_sequence is padded alike training inputs with trailing 0s if needed\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            output = self.model(input_sequence)\n",
    "            return output[:, -1, :]\n",
    "\n",
    "\n",
    "dataset = MovieLensDataset(train)\n",
    "dataloader = DataLoader(dataset, batch_sampler=SequenceSampler(dataset, batch_size=64), collate_fn=dataset.collate_fn)\n",
    "model = LanguageModel(vocab_size=1683)\n",
    "trainer = L.Trainer(max_epochs=100)\n",
    "trainer.fit(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at 10: 0.016568047337278107\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "model.eval()\n",
    "test_dataset = MovieLensDataset(test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_sampler=SequenceSampler(test_dataset, batch_size=1))\n",
    "\n",
    "acc_at_10 = []\n",
    "for batch in test_dataloader:\n",
    "    input, target = batch\n",
    "    target = target[0].item()\n",
    "    # get next target\n",
    "    output = model.predict_next(input)\n",
    "    topk = torch.topk(output, 10).indices.tolist()[0]\n",
    "    acc_at_10.append(target in topk)\n",
    "\n",
    "\n",
    "print(f\"Accuracy at 10: {sum(acc_at_10) / len(acc_at_10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some sample predictions\n",
    "\n",
    "import random\n",
    "random.shuffle(test)\n",
    "\n",
    "for i in range(10):\n",
    "    input = torch.tensor(test[i][:-1]).unsqueeze(0)\n",
    "    target = test[i][0]\n",
    "\n",
    "    output = model.predict_next(input)[0]\n",
    "    topk = torch.topk(output, 10).indices.tolist()\n",
    "\n",
    "    print(\"Last three movies watched:\")\n",
    "    for movie in input[0][-3:]:\n",
    "        print(movie_dict[movie.item()])\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"Next movie to watch:\")\n",
    "    print(movie_dict[target])\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"Top 10 recommendations:\")\n",
    "    for movie in topk:\n",
    "        print(movie_dict[movie])\n",
    "\n",
    "    print(\"--------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 recommendations:\n",
      "Ulee's Gold (1997)\n",
      "M (1931)\n",
      "Candidate, The (1972)\n",
      "Mulholland Falls (1996)\n",
      "Last Supper, The (1995)\n",
      "Ransom (1996)\n",
      "Spawn (1997)\n",
      "Lone Star (1996)\n",
      "Basic Instinct (1992)\n",
      "Full Monty, The (1997)\n"
     ]
    }
   ],
   "source": [
    "input = torch.tensor([1, 94, 261, 422, 477])\n",
    "out = model.predict_next(input.unsqueeze(0))\n",
    "\n",
    "recommendations = torch.topk(out, 10).indices.squeeze().tolist()\n",
    "\n",
    "print(\"Top 10 recommendations:\")\n",
    "for movie in recommendations:\n",
    "    print(movie_dict[movie])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
